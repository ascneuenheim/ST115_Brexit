{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74e6f76",
   "metadata": {},
   "source": [
    "# Question 1 - Applications/Offers/Entrances for departments by nationality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55e5ff",
   "metadata": {},
   "source": [
    "The data used came from the following website: https://public.tableau.com/app/profile/lseplanningdivision/vizzes. This contains all the official statistics on students which LSE publishes. Specifically, we used the \"LSE applications, Offers and Entrants\" tableau which breaks down the applications for various departments for both levels of study over the past 5 years by country.\n",
    "\n",
    "The problem with this data source was that the website is not dynmaically generated and therefore we can not use BeautifulSoup to scrape the data. We further tried to obtain the data via FOI (Freedom of Information) but unfortunately due to restricted access to the data this was not a possibility.\n",
    "\n",
    "Therefore, to circumvent this issue we downloaded the data as a pdf. Then we used the tabula and jpype modules to convert this pdf into a csv. \n",
    "\n",
    "IMPORTANT: To run this code you have to have Java installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75757d41",
   "metadata": {},
   "source": [
    "Converting the pdf to csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b04fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import tabula\n",
    "import jpype\n",
    "\n",
    "\n",
    "# URL of the PDF download link\n",
    "#pdf_download_url = \"https://public.tableau.com/vizql/w/ApplicationsOffersandEntrants/v/About/tempfile/sessions/6CFB072D66F548F0BD680B6D8269BD44-0:0/?key=2889495739&keepfile=yes&attachment=yes\"\n",
    "#pdf_download_url = \"https://public.tableau.com/vizql/w/LSEStatisticsonStudents/v/TableA/tempfile/sessions/E54A8D2F36614981A37995A0DF6C69BC-0:0/?key=674594458&keepfile=yes&attachment=yes\"\n",
    "# Send a GET request to download the PDF file\n",
    "#response = requests.get(pdf_download_url)\n",
    "\n",
    "# Save the content of the response to a file with .pdf extension\n",
    "file_path = \"Data/LSE_Students_acceptance_program_v1.pdf\"\n",
    "#with open(file_path, \"wb\") as f:\n",
    "#    f.write(response.content)\n",
    "\n",
    "# Extract the table from the PDF file\n",
    "tables = tabula.read_pdf(file_path, pages='all', multiple_tables=True)\n",
    "\n",
    "# Check if any tables are extracted\n",
    "if tables:\n",
    "    # Concatenate all tables into one DataFrame if there are multiple\n",
    "    combined_table = pd.concat(tables, ignore_index=True)\n",
    "    \n",
    "    # Save the combined\n",
    "    \n",
    "    table to a CSV file\n",
    "    csv_file = \"Data/data_table.csv\"\n",
    "    combined_table.to_csv(csv_file, index=False)\n",
    "    print(f\"Table saved to {csv_file}\")\n",
    "else:\n",
    "    print(\"No table found in the PDF file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72801ed",
   "metadata": {},
   "source": [
    "Notably the data extracted above will not only be used for question 1 but constantly throughout the project. Therefore, it arguably is our most important data source as it contains all the information regarding applications from different countries for the last five years which forms the basis of our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884fab8e",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a2fc4",
   "metadata": {},
   "source": [
    "The salary data was systematically gathered from the individual course websites using the Selenium and Beautiful Soup modules. Due to structural differences between undergraduate and postgraduate course websites, separate scraping scripts were used for each category. Initially, both scripts navigate to the central course overview website, where they apply filters to select either undergraduate or postgraduate courses. Subsequently, the scripts access each course's specific page from the listings.\n",
    "\n",
    "On reaching a course's individual page, the scripts are programmed to extract essential information, such as the name of the course, the department to which it belongs, and the median salary of graduates 15 months after completing the course. Upon extracting this information, going back to the main page and reaching the end of the page, the scripts automatically find and activate the “next” button to proceed to subsequent pages, ensuring a comprehensive capture of available data.\n",
    "\n",
    "For postgraduate courses, while the overall procedure remains the same, the specific HTML elements from which data is extracted differ due to variations in page layout. BeautifulSoup is used across both scripts to parse HTML content and accurately locate and extract the targeted data elements. The extracted data is then compiled and stored in a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6ec67",
   "metadata": {},
   "source": [
    "Undergraduate courses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e674eab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Results saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')  \n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Define URL\n",
    "initial_url = \"https://www.lse.ac.uk/programmes/search-courses?studyType=0%2F1%2F26%2F85%2F86\"\n",
    "driver.get(initial_url)\n",
    "\n",
    "course_data = []\n",
    "\n",
    "#Continue scraping until all pages are visited\n",
    "while True:\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'h2.card__title a'))\n",
    "    )\n",
    "    course_links = [elem.get_attribute('href') for elem in driver.find_elements(By.CSS_SELECTOR, 'h2.card__title a')]\n",
    "\n",
    "    for link in course_links:\n",
    "        driver.get(link)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "        #Extract department name\n",
    "        try:\n",
    "            department_section = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'why-study-with-us')))\n",
    "            soup = BeautifulSoup(department_section.get_attribute('outerHTML'), 'html.parser')\n",
    "            department_titles = [title.text.strip() for title in soup.find_all('h2', class_='card__title')]\n",
    "            department = ', '.join(department_titles) if department_titles else \"Department not found.\"\n",
    "        except TimeoutException:\n",
    "            department = \"Department not found.\"\n",
    "\n",
    "        #Extract course name\n",
    "        try:\n",
    "            course_name = driver.find_element(By.CSS_SELECTOR, 'h1.hero__title span').text.strip()\n",
    "        except NoSuchElementException:\n",
    "            course_name = \"Course name not found.\"\n",
    "\n",
    "        #Extract median salary\n",
    "        try:\n",
    "            salary_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'section#graduate-destinations div.salary'))\n",
    "            )\n",
    "            median_salary = salary_element.text.strip()\n",
    "        except TimeoutException:\n",
    "            median_salary = \"Salary not found.\"\n",
    "\n",
    "        course_data.append((course_name, median_salary, department))\n",
    "\n",
    "        #Navigate back to the list page\n",
    "        driver.get(initial_url)\n",
    "\n",
    "    #Attempt to move to the next page\n",
    "    try:\n",
    "        next_page_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(@class, 'next')]//button[contains(., 'Next')]\")))\n",
    "        next_page_button.click()\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        initial_url = driver.current_url\n",
    "    except TimeoutException:\n",
    "        break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "#save data to CSV\n",
    "csv_path = 'Data/output_median_salary.csv'\n",
    "with open(csv_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Course Name', 'Median Salary', 'Department'])\n",
    "    writer.writerows(course_data)\n",
    "\n",
    "print(\"Data extraction complete. Results saved to 'output_median_salary.csv'.\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a79445b",
   "metadata": {},
   "source": [
    "Postgraduate courses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2509c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Results added to 'output2804.csv'.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "#Set up the WebDriver for Chrome\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "courses_info = []\n",
    "\n",
    "#Define the base URL filtered for postgraduate courses\n",
    "base_url = \"https://www.lse.ac.uk/programmes/search-courses?studyType=0%2F1%2F26%2F85%2F87\"\n",
    "driver.get(base_url)\n",
    "current_page_url = base_url\n",
    "\n",
    "while True:\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'h2.card__title a')))\n",
    "    course_elements = driver.find_elements(By.CSS_SELECTOR, 'h2.card__title a')\n",
    "    courses = [(elem.get_attribute('href'), elem.text) for elem in course_elements]\n",
    "\n",
    "    for course_link, _ in courses:\n",
    "        driver.get(course_link)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        \n",
    "        #Extract course details using BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        #Extract course name\n",
    "        course_name_elem = soup.find('h1', class_='pageTitle')\n",
    "        course_name = course_name_elem.text.strip() if course_name_elem else \"Course name not found.\"\n",
    "        \n",
    "        #Extract department name\n",
    "        department_elem = soup.find('li', class_='keyDetails__item--dept')\n",
    "        department = department_elem.text.strip() if department_elem else \"Department not found.\"\n",
    "        \n",
    "        #Extract median salary\n",
    "        median_salary = \"Salary not found.\"\n",
    "        careers_accordion = soup.find('h1', class_='accordion__title', string=lambda text: 'Careers' in text if text else False)\n",
    "        if careers_accordion:\n",
    "            careers_content = careers_accordion.find_next('div', class_='accordion__content')\n",
    "            if careers_content:\n",
    "                salary_tag = careers_content.find('strong', string=lambda text: \"Median salary\" in text if text else False)\n",
    "                if salary_tag and salary_tag.next_sibling:\n",
    "                    median_salary = salary_tag.next_sibling.strip()\n",
    "\n",
    "        #Store extracted data\n",
    "        courses_info.append((course_name, median_salary, department))\n",
    "\n",
    "        #Navigate back to current page URL\n",
    "        driver.get(current_page_url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'h2.card__title a')))\n",
    "\n",
    "    #Navigate to the next page if possible\n",
    "    try:\n",
    "        next_page_button = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//li[contains(@class, 'next')]//button[contains(., 'Next')]\")))\n",
    "        next_page_button.click()\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        current_page_url = driver.current_url\n",
    "    except TimeoutException:\n",
    "        break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "#Save results to a CSV file\n",
    "csv_file_path = 'Data/output_median_salary.csv'\n",
    "file_exists = os.path.isfile(csv_file_path) \n",
    "\n",
    "with open(csv_file_path, 'a', newline='') as file: \n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(['Course Name', 'Median Salary', 'Department'])\n",
    "    writer.writerows(courses_info)\n",
    "\n",
    "print(\"Data extraction complete. Results added to 'output_median_salary.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419f680",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e66afa",
   "metadata": {},
   "source": [
    "This Question aims to relate the Brexit induced changes at LSE to economic characteristics of the countries students applied from. We thus acquire the GDP per capita as well as the population for every country. We conduct a series of operations to gather GDP data by country from a designated webpage, integrate it with nationality information from a CSV file, and then store the merged data in a new CSV file. The original data is from the World Bank and portraits the data from 2022.  A  function iterates over the HTML elements, retrieving the information, namely country name, GDP, population, and GDP per capita.\n",
    "Subsequently, another function combines the scraped GDP data with nationality information fetched from the CSV file we obtained in Question 1. It reads the CSV file containing country-nationality mappings, matches countries with their corresponding nationalities, and creates a merged dataset. Upon execution, the code facilitates the compilation of comprehensive GDP data by country, along with their respective nationalities, for further analysis or visualization purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cde0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_gdp_by_country():\n",
    "    url = \"https://www.worldometers.info/gdp/gdp-by-country/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    rows = soup.find_all('tr')\n",
    "    gdp_by_country_data = []\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) >= 7:\n",
    "            country = cells[1].text.strip()\n",
    "            gdp = cells[2].text.strip().replace('$', '').replace(',', '')\n",
    "            population = cells[5].text.strip().replace(',', '')\n",
    "            gdp_per_capita = cells[6].text.strip().replace('$', '').replace(',', '')\n",
    "            gdp_by_country_data.append((country, gdp, population, gdp_per_capita))\n",
    "    \n",
    "    return gdp_by_country_data\n",
    "\n",
    "def merge_with_country_nationality_mapping(gdp_data, country_nationality_file):\n",
    "    country_nationality_map = {}\n",
    "    with open(country_nationality_file, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            country_name = row['en_short_name']\n",
    "            nationality = row['nationality']\n",
    "            country_nationality_map[country_name] = nationality\n",
    "\n",
    "    merged_data = []\n",
    "    for country, gdp, population, gdp_per_capita in gdp_data:\n",
    "        if country in country_nationality_map:\n",
    "            nationality = country_nationality_map[country]\n",
    "            merged_data.append((country, nationality, gdp, population, gdp_per_capita))\n",
    "        else:\n",
    "            merged_data.append((country, '', gdp, population, gdp_per_capita))\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "def write_to_csv(data, csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Country', 'Nationality', 'GDP', 'Population', 'GDP per Capita'])\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    gdp_by_country_data = scrape_gdp_by_country()\n",
    "    if gdp_by_country_data:\n",
    "        merged_data = merge_with_country_nationality_mapping(gdp_by_country_data, 'Data/countries.csv')\n",
    "        write_to_csv(merged_data, 'Data/merged_gdp_data.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37129b01",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70d946",
   "metadata": {},
   "source": [
    "For question 4 we decided to use publications data to proxy a department’s research effort. A reliable data source for this was “LSE Research Online” (https://eprints.lse.ac.uk/cgi/search/advanced) with which it is possible to search through LSE’s publications. Especially useful was the fact that here it is possible to directly specify the division whose publications you want to search.\n",
    "As this is an extensive process we again decided to use the selenium webdriver to automate the web browsing and thus the data acquisition process. \n",
    "\n",
    "The list of departments that was initialized in the code are the ones for whose publications were searched. Specifically the code attempts to select the department from a dropdown menu on the page. For those where the department is available it then enters a year into the search field and submits the form to load the search results for that year. These are then stored in a dataframe for the corresponding department and year. Through this the code collects the number of publications from each department in the initialized list for each year from 2019 to 2023 and then stores the results in a csv. Notably the dataframe is saved to the csv file incrementally as the automated webdriving is a long process and can be interrupted frequently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87781168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping completed and saved to 'Data/department_yearly_results.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Department</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Geography &amp; Environment</td>\n",
       "      <td>117</td>\n",
       "      <td>175</td>\n",
       "      <td>177</td>\n",
       "      <td>170</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Philosophy, Logic and Scientific Method</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>63</td>\n",
       "      <td>66</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Psychological and Behavioural Science</td>\n",
       "      <td>77</td>\n",
       "      <td>125</td>\n",
       "      <td>152</td>\n",
       "      <td>156</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Government</td>\n",
       "      <td>106</td>\n",
       "      <td>126</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Law</td>\n",
       "      <td>85</td>\n",
       "      <td>118</td>\n",
       "      <td>146</td>\n",
       "      <td>92</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Social Policy</td>\n",
       "      <td>112</td>\n",
       "      <td>124</td>\n",
       "      <td>116</td>\n",
       "      <td>138</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mathematics</td>\n",
       "      <td>53</td>\n",
       "      <td>91</td>\n",
       "      <td>56</td>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Economic History</td>\n",
       "      <td>49</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sociology</td>\n",
       "      <td>45</td>\n",
       "      <td>54</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>International History</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Statistics</td>\n",
       "      <td>42</td>\n",
       "      <td>63</td>\n",
       "      <td>54</td>\n",
       "      <td>84</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Management</td>\n",
       "      <td>87</td>\n",
       "      <td>114</td>\n",
       "      <td>108</td>\n",
       "      <td>112</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>International Relations</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>93</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Anthropology</td>\n",
       "      <td>36</td>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Economics</td>\n",
       "      <td>114</td>\n",
       "      <td>194</td>\n",
       "      <td>139</td>\n",
       "      <td>142</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Language Centre</td>\n",
       "      <td>Timeout or no results</td>\n",
       "      <td>Timeout or no results</td>\n",
       "      <td>Timeout or no results</td>\n",
       "      <td>Timeout or no results</td>\n",
       "      <td>Timeout or no results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Finance</td>\n",
       "      <td>22</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Methodology</td>\n",
       "      <td>39</td>\n",
       "      <td>76</td>\n",
       "      <td>58</td>\n",
       "      <td>64</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>School of Public Policy</td>\n",
       "      <td>18</td>\n",
       "      <td>34</td>\n",
       "      <td>50</td>\n",
       "      <td>26</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>European Institute</td>\n",
       "      <td>63</td>\n",
       "      <td>147</td>\n",
       "      <td>86</td>\n",
       "      <td>100</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Media and Communications</td>\n",
       "      <td>89</td>\n",
       "      <td>125</td>\n",
       "      <td>141</td>\n",
       "      <td>133</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Health Policy</td>\n",
       "      <td>101</td>\n",
       "      <td>149</td>\n",
       "      <td>178</td>\n",
       "      <td>189</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>International Development</td>\n",
       "      <td>111</td>\n",
       "      <td>127</td>\n",
       "      <td>104</td>\n",
       "      <td>89</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Gender Studies</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Department                   2019  \\\n",
       "0                   Geography & Environment                    117   \n",
       "1   Philosophy, Logic and Scientific Method                     64   \n",
       "2     Psychological and Behavioural Science                     77   \n",
       "3                                Government                    106   \n",
       "4                                       Law                     85   \n",
       "5                             Social Policy                    112   \n",
       "6                               Mathematics                     53   \n",
       "7                          Economic History                     49   \n",
       "8                                 Sociology                     45   \n",
       "9                     International History                     41   \n",
       "10                               Statistics                     42   \n",
       "11                               Management                     87   \n",
       "12                  International Relations                     85   \n",
       "13                             Anthropology                     36   \n",
       "14                                Economics                    114   \n",
       "15                          Language Centre  Timeout or no results   \n",
       "16                               Accounting                     22   \n",
       "17                                  Finance                     22   \n",
       "18                              Methodology                     39   \n",
       "19                  School of Public Policy                     18   \n",
       "20                       European Institute                     63   \n",
       "21                 Media and Communications                     89   \n",
       "22                            Health Policy                    101   \n",
       "23                International Development                    111   \n",
       "24                           Gender Studies                     19   \n",
       "\n",
       "                     2020                   2021                   2022  \\\n",
       "0                     175                    177                    170   \n",
       "1                      60                     63                     66   \n",
       "2                     125                    152                    156   \n",
       "3                     126                     85                    100   \n",
       "4                     118                    146                     92   \n",
       "5                     124                    116                    138   \n",
       "6                      91                     56                     64   \n",
       "7                      45                     39                     38   \n",
       "8                      54                     59                     57   \n",
       "9                      43                     37                     35   \n",
       "10                     63                     54                     84   \n",
       "11                    114                    108                    112   \n",
       "12                     90                     91                     93   \n",
       "13                     51                     47                     57   \n",
       "14                    194                    139                    142   \n",
       "15  Timeout or no results  Timeout or no results  Timeout or no results   \n",
       "16                     16                     25                     30   \n",
       "17                     37                     33                     30   \n",
       "18                     76                     58                     64   \n",
       "19                     34                     50                     26   \n",
       "20                    147                     86                    100   \n",
       "21                    125                    141                    133   \n",
       "22                    149                    178                    189   \n",
       "23                    127                    104                     89   \n",
       "24                     21                     20                     18   \n",
       "\n",
       "                     2023  \n",
       "0                     145  \n",
       "1                      61  \n",
       "2                     146  \n",
       "3                     113  \n",
       "4                     117  \n",
       "5                     142  \n",
       "6                      50  \n",
       "7                      53  \n",
       "8                      50  \n",
       "9                      32  \n",
       "10                     94  \n",
       "11                     95  \n",
       "12                    101  \n",
       "13                     50  \n",
       "14                    134  \n",
       "15  Timeout or no results  \n",
       "16                     14  \n",
       "17                     30  \n",
       "18                     72  \n",
       "19                     41  \n",
       "20                     93  \n",
       "21                    137  \n",
       "22                    209  \n",
       "23                     80  \n",
       "24                     24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select, WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "search_url = \"https://eprints.lse.ac.uk/cgi/search/advanced\"\n",
    "driver.get(search_url)\n",
    "\n",
    "#Prepare a DataFrame for storing results\n",
    "columns = ['Department'] + [str(year) for year in range(2019, 2024)]\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "#List of departments\n",
    "departments = [\n",
    "    'Geography & Environment', 'Philosophy, Logic and Scientific Method', \n",
    "    'Psychological and Behavioural Science', 'Government', 'Law', \n",
    "    'Social Policy', 'Mathematics', 'Economic History', 'Sociology', \n",
    "    'International History', 'Statistics', 'Management', 'International Relations', \n",
    "    'Anthropology', 'Economics', 'Language Centre', 'Accounting', 'Finance', \n",
    "    'Methodology', 'School of Public Policy', \n",
    "    'European Institute', 'Media and Communications', 'Health Policy', \n",
    "    'International Development', 'Gender Studies'\n",
    "]\n",
    "\n",
    "#Process each department\n",
    "for department in departments:\n",
    "    row_data = {'Department': department}\n",
    "    driver.get(search_url)  #Navigate back to the main search page for each department\n",
    "    try:\n",
    "        divisions_select = Select(driver.find_element(By.ID, \"divisions\"))  #Locate the dropdown again\n",
    "        divisions_select.select_by_visible_text(department)\n",
    "        available = True\n",
    "    except NoSuchElementException:\n",
    "        available = False\n",
    "        print(f\"Department {department} not found.\")\n",
    "    \n",
    "    if available:\n",
    "        for year in range(2019, 2024):\n",
    "            try:\n",
    "                wait = WebDriverWait(driver, 2)\n",
    "                date_input = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"input[name='date']\")))\n",
    "                date_input.clear()\n",
    "                date_input.send_keys(str(year))\n",
    "                date_input.send_keys(Keys.RETURN)\n",
    "\n",
    "                # Wait for the page to load and scrape the total results\n",
    "                total_results_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"span.ep_search_number\")))\n",
    "                total_results = total_results_elements[-1].text  #Get the text of the last element\n",
    "                row_data[str(year)] = total_results\n",
    "            except NoSuchElementException:\n",
    "                row_data[str(year)] = 'Element not found'\n",
    "            except TimeoutException:\n",
    "                row_data[str(year)] = 'Timeout or no results'\n",
    "            driver.get('https://eprints.lse.ac.uk/cgi/search/archive/advanced')\n",
    "            divisions_select = Select(driver.find_element(By.ID, \"divisions\"))  #Locate the dropdown again\n",
    "            divisions_select.select_by_visible_text(department)\n",
    "    #Append the results of this department to the DataFrame and save incrementally\n",
    "    new_row = pd.DataFrame([row_data])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    results_df.to_csv('Data/department_yearly_results.csv', index=False)\n",
    "\n",
    "#Close the browser\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data scraping completed and saved to 'Data/department_yearly_results.csv'.\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc0bda",
   "metadata": {},
   "source": [
    "This question also required the data on LSE's fees. LSE's website contains a section specific for this (https://info.lse.ac.uk/staff/divisions/Planning-Division/Table-of-Fees). Here the fees for each academic year from 2017/18 to the upcoming one (2024/2025) is available in the forms of downloadable pdfs.\n",
    "\n",
    "Again we used the selenium webdriver to automate the process of downloading these pdfs. As these had a inconsistent pattern in names we ensured to label the names in a way that is easier interpretable which was especially important for later on, once this data was cleaned and visualised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd3d952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed Table-of-fees-2024-25-20Feb24-Updated-Home-PGR-fee.pdf to Fees2024.pdf\n",
      "Renamed Table-of-fees-2023-24-7Nov23.pdf to Fees2023.pdf\n",
      "Renamed Comb2022ToF-Final-19July23.pdf to Fees2022.pdf\n",
      "Renamed ToF-3Aug21FinalComb.pdf to Fees2021.pdf\n",
      "Renamed 2020-Table-of-Fees-25Jun20.pdf to Fees2020.pdf\n",
      "Renamed 2019-Table-of-Fees.pdf to Fees2019.pdf\n",
      "Renamed 2018-19-Fees-Table.pdf to Fees2018.pdf\n",
      "Renamed 2017-18-Fees-Table.pdf to Fees2017.pdf\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "download_dir_relative = 'Data/TuitionFees'\n",
    "download_dir_absolute = os.path.abspath(download_dir_relative)\n",
    "if not os.path.exists(download_dir_absolute):\n",
    "    os.makedirs(download_dir_absolute)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"prefs\", {\n",
    "    \"download.default_directory\": download_dir_absolute,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "})\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "#Base URL and class name for PDF downloads\n",
    "base_url = 'https://info.lse.ac.uk/staff/divisions/Planning-Division/Table-of-Fees'\n",
    "class_name = 'sys_21'\n",
    "driver.get(base_url)\n",
    "time.sleep(2)  # Allow time for the page to load\n",
    "links = driver.find_elements(By.CLASS_NAME, class_name)\n",
    "\n",
    "for link in links:\n",
    "    href = link.get_attribute('href')\n",
    "    if \"Fee-approval-cycle-2024.pdf\" in href:\n",
    "        continue\n",
    "    if href and href.endswith('.pdf'):\n",
    "        original_filename = href.split('/')[-1]\n",
    "        # Extract year from the file name\n",
    "        four_digit_year_match = re.search(r'(\\d{4})', original_filename)\n",
    "        year = four_digit_year_match.group(1) if four_digit_year_match else None\n",
    "        if not year:\n",
    "            two_digit_year_match = re.search(r'(\\d{2})', original_filename)\n",
    "            year = '20' + two_digit_year_match.group(1) if two_digit_year_match else None\n",
    "        \n",
    "        if year:\n",
    "            driver.execute_script(f\"window.open('{href}');\")\n",
    "            time.sleep(1)  \n",
    "            # Path handling for renaming the file\n",
    "            original_path = os.path.join(download_dir_absolute, original_filename)\n",
    "            new_filename = f\"Fees{year}.pdf\"\n",
    "            new_path = os.path.join(download_dir_absolute, new_filename)\n",
    "            os.rename(original_path, new_path)\n",
    "            print(f\"Renamed {original_filename} to {new_filename}\")\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
