{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516be433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Sets up the WebDriver for Chrome.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox')  # Bypass OS security model\n",
    "    options.add_argument('--disable-dev-shm-usage')  # Overcome limited resource problems\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def print_department_text(html_content):\n",
    "    \"\"\"Extracts and returns the text after the <h2 class=\"card__title\"> tag.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    card_titles = soup.find_all('h2', class_='card__title')\n",
    "    departments = [title.text.strip() for title in card_titles]\n",
    "    return departments\n",
    "\n",
    "def print_why_study_with_us_section(driver, course_link):\n",
    "    \"\"\"Returns the HTML content of the 'why study with us' section.\"\"\"\n",
    "    driver.get(course_link)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "    try:\n",
    "        why_study_with_us_section = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, 'why-study-with-us'))\n",
    "        )\n",
    "        html_content = why_study_with_us_section.get_attribute('outerHTML')\n",
    "        return print_department_text(html_content)\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to locate or extract the 'why study with us' section.\")\n",
    "        return []\n",
    "\n",
    "# Setup WebDriver\n",
    "driver = setup_driver()\n",
    "initial_url = \"https://www.lse.ac.uk/programmes/search-courses?studyType=0%2F1%2F26%2F85%2F86\"\n",
    "driver.get(initial_url)\n",
    "\n",
    "def process_course_page(course_link):\n",
    "    \"\"\" Visits each course link and extracts the course name, median salary, and department. \"\"\"\n",
    "    departments = print_why_study_with_us_section(driver, course_link)\n",
    "    department = ', '.join(departments) if departments else \"Department not found.\"\n",
    "    \n",
    "    try:\n",
    "        course_name = driver.find_element(By.CSS_SELECTOR, 'h1.hero__title span').text.strip()\n",
    "    except NoSuchElementException:\n",
    "        course_name = \"Course name not found.\"\n",
    "    try:\n",
    "        salary_div = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'section#graduate-destinations div.salary')))\n",
    "        median_salary = salary_div.text.strip()\n",
    "    except TimeoutException:\n",
    "        median_salary = \"Salary not found.\"\n",
    "\n",
    "    return course_name, median_salary, department\n",
    "\n",
    "def navigate_to_next_page():\n",
    "    \"\"\"Navigates to the next page if possible.\"\"\"\n",
    "    try:\n",
    "        next_page_button = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//li[contains(@class, 'next')]//button[contains(., 'Next')]\")))\n",
    "        next_page_button.click()\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        return driver.current_url\n",
    "    except TimeoutException:\n",
    "        return None\n",
    "\n",
    "courses_info = []\n",
    "current_page_url = initial_url\n",
    "\n",
    "while True:\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'h2.card__title a')))\n",
    "    courses = [(elem.get_attribute('href'), elem.text) for elem in driver.find_elements(By.CSS_SELECTOR, 'h2.card__title a')]\n",
    "    for course_link, _ in courses:\n",
    "        course_name, median_salary, department = process_course_page(course_link)\n",
    "        courses_info.append((course_name, median_salary, department))\n",
    "        driver.get(current_page_url)\n",
    "    new_page_url = navigate_to_next_page()\n",
    "    if new_page_url:\n",
    "        current_page_url = new_page_url\n",
    "    else:\n",
    "        break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save results to a CSV file\n",
    "csv_file_path = 'output.csv'\n",
    "with open(csv_file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Course Name', 'Median Salary', 'Department'])\n",
    "    writer.writerows(courses_info)\n",
    "\n",
    "print(\"Data extraction complete. Results saved to CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6644d3f",
   "metadata": {},
   "source": [
    "Postgraduate Courses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beace6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Sets up the WebDriver for Chrome.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def process_course_page(driver, course_link):\n",
    "    \"\"\"Visits each course link and extracts the course name, median salary, and department for postgraduate courses.\"\"\"\n",
    "    driver.get(course_link)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Extracting course name\n",
    "    course_name_elem = soup.find('h1', class_='pageTitle')\n",
    "    course_name = course_name_elem.text.strip() if course_name_elem else \"Course name not found.\"\n",
    "\n",
    "    # Finding department\n",
    "    department_elem = soup.find('li', class_='keyDetails__item--dept')\n",
    "    department = department_elem.text.strip() if department_elem else \"Department not found.\"\n",
    "\n",
    "    # Extracting median salary directly from the Careers accordion content\n",
    "    # Extracting median salary directly from the Careers accordion content\n",
    "    median_salary = \"Salary not found.\"\n",
    "    careers_accordion = soup.find('h1', class_='accordion__title', string=lambda text: 'Careers' in text if text else False)\n",
    "    if careers_accordion:\n",
    "        careers_content = careers_accordion.find_next('div', class_='accordion__content')\n",
    "        if careers_content:\n",
    "            salary_tag = careers_content.find('strong', string=lambda text: \"Median salary\" in text if text else False)\n",
    "            if salary_tag:\n",
    "                median_salary = salary_tag.next_sibling.strip() if salary_tag.next_sibling else \"Salary not found.\"\n",
    "\n",
    "    print(median_salary)\n",
    "    return course_name, median_salary, department\n",
    "\n",
    "\n",
    "# Main scraping function\n",
    "def scrape_courses(base_url):\n",
    "    driver = setup_driver()\n",
    "    driver.get(base_url)\n",
    "    courses_info = []\n",
    "    current_page_url = base_url  # Initialize current page URL\n",
    "\n",
    "    while True:\n",
    "        # Wait for the course links to be visible and then collect them\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'h2.card__title a')))\n",
    "        course_elements = driver.find_elements(By.CSS_SELECTOR, 'h2.card__title a')\n",
    "        courses = [(elem.get_attribute('href'), elem.text) for elem in course_elements]\n",
    "\n",
    "        for course_link, _ in courses:\n",
    "            driver.get(course_link)\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            course_info = process_course_page(driver, course_link)\n",
    "            courses_info.append(course_info)\n",
    "            driver.get(current_page_url)  # Go back to the current list page, not the base URL\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'h2.card__title a')))  # Wait until all course links are visible again\n",
    "\n",
    "        new_page_url = navigate_to_next_page(driver)\n",
    "        if new_page_url:\n",
    "            current_page_url = new_page_url  # Update the current page URL\n",
    "            driver.get(new_page_url)  # Navigate to the next page\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))  # Ensure the page is loaded\n",
    "        else:\n",
    "            break  # Exit the loop if there are no more pages\n",
    "\n",
    "    driver.quit()\n",
    "    return courses_info\n",
    "\n",
    "def navigate_to_next_page(driver):\n",
    "    \"\"\"Navigates to the next page if possible.\"\"\"\n",
    "    try:\n",
    "        next_page_button = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//li[contains(@class, 'next')]//button[contains(., 'Next')]\")))\n",
    "        next_page_button.click()\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        return driver.current_url\n",
    "    except TimeoutException:\n",
    "        return None\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"Appends the data to a CSV file or creates it if it doesn't exist.\"\"\"\n",
    "    file_exists = os.path.isfile(filename)  # Check if the file already exists\n",
    "\n",
    "    with open(filename, 'a', newline='') as file:  # Open the file in append mode\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writerow(['Course Name', 'Median Salary', 'Department'])  # Write header only if the file does not exist\n",
    "        \n",
    "        writer.writerows(data)\n",
    "\n",
    "\n",
    "# URL for postgraduate courses\n",
    "postgraduate_url = \"https://www.lse.ac.uk/programmes/search-courses?studyType=0%2F1%2F26%2F85%2F87\"\n",
    "postgraduate_courses = scrape_courses(postgraduate_url)\n",
    "save_to_csv(postgraduate_courses, 'output.csv')\n",
    "print(\"Data extraction complete. Results added to 'output.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207fb95c",
   "metadata": {},
   "source": [
    "Cleaning of webscraping output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ba544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV file\n",
    "data_path = 'output.csv'\n",
    "data = pd.read_csv(data_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Simplify department names by removing \"Department of \"\n",
    "data['Department'] = data['Department'].str.replace('Department of ', '')\n",
    "\n",
    "# Replace \"Salary not found\" with NaN\n",
    "data['Median Salary'] = data['Median Salary'].replace('Salary not found', pd.NA)\n",
    "\n",
    "# Convert salary values to numeric, removing non-numeric characters like £ and commas\n",
    "data['Median Salary'] = pd.to_numeric(data['Median Salary'].str.replace('[£,]', '', regex=True), errors='coerce')\n",
    "\n",
    "# Determine median salary to categorize into 'High' and 'Low' groups\n",
    "median_salary = data['Median Salary'].median()\n",
    "data['Salary Group'] = data['Median Salary'].apply(lambda x: 'High' if x >= median_salary else 'Low')\n",
    "\n",
    "# Remove duplicate rows\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "clean_data_path = 'cleaned_output.csv'\n",
    "data.to_csv(clean_data_path, index=False)\n",
    "\n",
    "print(f\"Data has been cleaned, duplicates removed, and saved to {clean_data_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd2875",
   "metadata": {},
   "source": [
    "Basic plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5338cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('PhilipEUCleaned.csv')\n",
    "\n",
    "# Define the years to aggregate data over\n",
    "years = ['2019', '2020', '2021', '2022', '2023']\n",
    "\n",
    "# Prepare the aggregation for applications, offers, and entrances by year and salary group\n",
    "agg_columns = {}\n",
    "for year in years:\n",
    "    agg_columns[f'Applications {year}'] = 'sum'\n",
    "    agg_columns[f'Offers {year}'] = 'sum'\n",
    "    agg_columns[f'Entrances {year}'] = 'sum'\n",
    "\n",
    "# Aggregating the data\n",
    "agg_data = data.groupby('Salary Group').agg(agg_columns).reset_index()\n",
    "\n",
    "# Calculate growth rate of applications year-over-year for each salary group\n",
    "for year in years[1:]:  # Start from 2020 to be able to calculate growth from 2019\n",
    "    previous_year = str(int(year) - 1)\n",
    "    agg_data[f'Applications Growth {year}'] = (\n",
    "        (agg_data[f'Applications {year}'] - agg_data[f'Applications {previous_year}']) / agg_data[f'Applications {previous_year}']) * 100\n",
    "\n",
    "# Prepare data for plotting growth rates\n",
    "growth_columns = [f'Applications Growth {year}' for year in years[1:]]  # Exclude 2019 as there's no previous year data\n",
    "melted_growth_data = agg_data.melt(id_vars=['Salary Group'], value_vars=growth_columns, var_name='Year_Type', value_name='Growth Rate')\n",
    "melted_growth_data['Year'] = melted_growth_data['Year_Type'].str.extract('(\\d+)').astype(int)\n",
    "melted_growth_data['Type'] = 'Applications Growth'\n",
    "\n",
    "# Combine growth data with other rate data for plotting\n",
    "combined_data = pd.concat([melted_growth_data, melted_data[melted_data['Type'].str.contains('Rate')]])\n",
    "\n",
    "# Plotting with applications growth rate\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 18))\n",
    "\n",
    "# Adjust plot settings\n",
    "types = ['Applications Growth', 'Offer Rate', 'Entrance Rate']\n",
    "colors = ['purple', 'green', 'red']\n",
    "for ax, rate_type in zip(axes.flatten(), types):\n",
    "    pivot_df = combined_data[combined_data['Type'] == rate_type].pivot(index='Year', columns='Salary Group', values='Growth Rate' if 'Growth' in rate_type else 'Rate')\n",
    "    pivot_df.plot(kind='bar', ax=ax, color=colors)\n",
    "    ax.set_title(f'{rate_type} Over the Years by Salary Group')\n",
    "    ax.set_ylabel('Growth Rate (%)' if 'Growth' in rate_type else 'Rate (%)')\n",
    "    ax.set_xlabel('Year')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2d43a",
   "metadata": {},
   "source": [
    "Regression of difference in applications on median salary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the CSV files\n",
    "file_eu_cleaned = pd.read_csv('PhilipEUCleaned.csv')\n",
    "file_output = pd.read_csv('data/PhilipOutput.csv')\n",
    "\n",
    "# Filter for only undergraduate courses in the EU cleaned data\n",
    "undergrad_eu_cleaned = file_eu_cleaned[file_eu_cleaned['Program'].str.contains(\"UG Degree\")]\n",
    "\n",
    "# Compute total applications for the years 2020 (pre-Brexit) and 2022 (post-Brexit)\n",
    "pre_brexit_apps = undergrad_eu_cleaned.groupby('Department')['Applications 2019'].sum()\n",
    "post_brexit_apps = undergrad_eu_cleaned.groupby('Department')['Applications 2023'].sum()\n",
    "\n",
    "# Calculate the difference in applications\n",
    "application_difference = post_brexit_apps - pre_brexit_apps\n",
    "\n",
    "# Get the median salary for each department from the second file, ensuring to match only undergraduate departments\n",
    "median_salaries = file_output[file_output['Course Name'].str.contains(\"BSc\")].groupby('Simple Department')['Median Salary'].median()\n",
    "\n",
    "# Join the application difference and median salary dataframes\n",
    "department_data = pd.DataFrame({\n",
    "    'Median Salary': median_salaries,\n",
    "    'Application Difference': application_difference,\n",
    "    'Total Pre-Brexit Applications': pre_brexit_apps,\n",
    "    'Total Post-Brexit Applications': post_brexit_apps\n",
    "}).dropna()\n",
    "\n",
    "# Add department names for plotting\n",
    "department_data['Department'] = department_data.index\n",
    "\n",
    "# Perform linear regression\n",
    "X = department_data['Median Salary'].values.reshape(-1, 1)\n",
    "y = department_data['Application Difference'].values\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions for the line\n",
    "department_data['Predicted Difference'] = model.predict(X)\n",
    "\n",
    "# Create the interactive plot\n",
    "fig = px.scatter(department_data, x='Median Salary', y='Application Difference', trendline='ols',\n",
    "                 labels={\n",
    "                     'Median Salary': 'Median Salary (£)',\n",
    "                     'Application Difference': 'Difference in Total Applications (2023 vs 2019)'\n",
    "                 },\n",
    "                 hover_data=['Department', 'Total Pre-Brexit Applications', 'Total Post-Brexit Applications'],\n",
    "                 title='Interactive Plot: Median Salary vs. Application Difference Post vs Pre Brexit for Undergraduate Courses')\n",
    "fig.update_traces(marker=dict(size=8),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866563e",
   "metadata": {},
   "source": [
    "Summary table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the CSV files\n",
    "file_eu_cleaned = pd.read_csv('PhilipEUCleaned.csv')\n",
    "file_output = pd.read_csv('data/PhilipOutput.csv')\n",
    "\n",
    "# Filter for only undergraduate courses in the EU cleaned data\n",
    "undergrad_eu_cleaned = file_eu_cleaned[file_eu_cleaned['Program'].str.contains(\"UG Degree\")]\n",
    "\n",
    "# Compute total applications for the years 2020 (pre-Brexit) and 2022 (post-Brexit)\n",
    "pre_brexit_apps = undergrad_eu_cleaned.groupby('Department')['Applications 2019'].sum()\n",
    "post_brexit_apps = undergrad_eu_cleaned.groupby('Department')['Applications 2023'].sum()\n",
    "\n",
    "# Calculate the difference in applications\n",
    "application_difference = post_brexit_apps - pre_brexit_apps\n",
    "\n",
    "# Get the median salary for each department from the second file, ensuring to match only undergraduate departments\n",
    "median_salaries = file_output[file_output['Course Name'].str.contains(\"BSc\")].groupby('Simple Department')['Median Salary'].median()\n",
    "\n",
    "# Join the application difference and median salary dataframes\n",
    "department_data = pd.DataFrame({\n",
    "    'Median Salary': median_salaries,\n",
    "    'Application Difference': application_difference,\n",
    "    'Total Pre-Brexit Applications': pre_brexit_apps,\n",
    "    'Total Post-Brexit Applications': post_brexit_apps\n",
    "}).dropna()\n",
    "\n",
    "# Add department names for indexing\n",
    "department_data['Department'] = department_data.index\n",
    "\n",
    "# Set up the X and y matrices for the regression, with an intercept added to X\n",
    "X = department_data[['Median Salary']]\n",
    "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
    "y = department_data['Application Difference']\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary of the regression\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f6c3e",
   "metadata": {},
   "source": [
    "Regression Discontinuity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "cleaned_data = pd.read_csv('PhilipEUCleaned.csv')\n",
    "\n",
    "# Aggregate application numbers by year and salary group\n",
    "applications_by_salary_group = cleaned_data.melt(\n",
    "    id_vars=['Nationality', 'Department', 'Program', 'Salary Group'],\n",
    "    value_vars=['Applications 2019', 'Applications 2020', 'Applications 2021', 'Applications 2022', 'Applications 2023'],\n",
    "    var_name='Year',\n",
    "    value_name='Applications'\n",
    ")\n",
    "applications_by_salary_group['Year'] = applications_by_salary_group['Year'].str.extract('(\\d+)').astype(int)\n",
    "applications_by_salary_group = applications_by_salary_group.groupby(['Year', 'Salary Group'])['Applications'].sum().reset_index()\n",
    "\n",
    "\n",
    "# Create rdf_high and rdf_low from the applications_by_salary_group\n",
    "rdf_high = applications_by_salary_group[applications_by_salary_group['Salary Group'] == 'High']\n",
    "rdf_low = applications_by_salary_group[applications_by_salary_group['Salary Group'] == 'Low']\n",
    "\n",
    "# Preparing data for piecewise linear regression fitting\n",
    "# Splitting the data based on the Brexit cutoff\n",
    "\n",
    "# Data before the Brexit cutoff\n",
    "pre_brexit_data_high = rdf_high[rdf_high['Year'] <= 2020]\n",
    "pre_brexit_data_low = rdf_low[rdf_low['Year'] <= 2020]\n",
    "\n",
    "# Data after the Brexit cutoff\n",
    "post_brexit_data_high = rdf_high[rdf_high['Year'] > 2020]\n",
    "post_brexit_data_low = rdf_low[rdf_low['Year'] > 2020]\n",
    "\n",
    "# Fit the models for each segment\n",
    "model_pre_high = sm.OLS(pre_brexit_data_high['Applications'], sm.add_constant(pre_brexit_data_high['Year'])).fit()\n",
    "model_post_high = sm.OLS(post_brexit_data_high['Applications'], sm.add_constant(post_brexit_data_high['Year'])).fit()\n",
    "\n",
    "model_pre_low = sm.OLS(pre_brexit_data_low['Applications'], sm.add_constant(pre_brexit_data_low['Year'])).fit()\n",
    "model_post_low = sm.OLS(post_brexit_data_low['Applications'], sm.add_constant(post_brexit_data_low['Year'])).fit()\n",
    "\n",
    "# Predict values for a smoother line\n",
    "fit_pre_high = model_pre_high.predict(sm.add_constant([2019, 2020]))\n",
    "fit_post_high = model_post_high.predict(sm.add_constant([2021, 2022, 2023]))\n",
    "\n",
    "fit_pre_low = model_pre_low.predict(sm.add_constant([2019, 2020]))\n",
    "fit_post_low = model_post_low.predict(sm.add_constant([2021, 2022, 2023]))\n",
    "\n",
    "extended_pre_brexit_years_high = np.array([2019, 2020, 2021])\n",
    "extended_pre_brexit_years_low = np.array([2019, 2020, 2021])\n",
    "extended_fit_pre_high = model_pre_high.predict(sm.add_constant(extended_pre_brexit_years_high))\n",
    "extended_fit_pre_low = model_pre_low.predict(sm.add_constant(extended_pre_brexit_years_low))\n",
    "\n",
    "# Plotting with the adjusted cutoff line, extrapolated pre-Brexit line, and gridlines\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# High salary departments\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(rdf_high['Year'], rdf_high['Applications'], color='blue')\n",
    "plt.plot(extended_pre_brexit_years_high, extended_fit_pre_high, color='red', label='Extended Pre-Brexit Fit')\n",
    "plt.plot([2021, 2022, 2023], fit_post_high, color='green', label='Post-Brexit Fit')\n",
    "plt.axvline(x=2021, color='black', linestyle='--', label='Brexit Cutoff (2021)')\n",
    "plt.title('High Salary Departments')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Applications')\n",
    "plt.xticks(range(2019, 2024))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Low salary departments\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(rdf_low['Year'], rdf_low['Applications'], color='green')\n",
    "plt.plot(extended_pre_brexit_years_low, extended_fit_pre_low, color='red', label='Extended Pre-Brexit Fit')\n",
    "plt.plot([2021, 2022, 2023], fit_post_low, color='blue', label='Post-Brexit Fit')\n",
    "plt.axvline(x=2021, color='black', linestyle='--', label='Brexit Cutoff (2021)')\n",
    "plt.title('Low Salary Departments')\n",
    "plt.xlabel('Year')\n",
    "plt.xticks(range(2019, 2024))\n",
    "plt.ylabel('Applications')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "cleaned_data = pd.read_csv('PhilipEUCleaned.csv')\n",
    "\n",
    "# Aggregate application numbers by year and salary group\n",
    "applications_by_salary_group = cleaned_data.melt(\n",
    "    id_vars=['Nationality', 'Department', 'Program', 'Salary Group'],\n",
    "    value_vars=['Applications 2019', 'Applications 2020', 'Applications 2021', 'Applications 2022', 'Applications 2023'],\n",
    "    var_name='Year',\n",
    "    value_name='Applications'\n",
    ")\n",
    "applications_by_salary_group['Year'] = applications_by_salary_group['Year'].str.extract('(\\d+)').astype(int)\n",
    "applications_by_salary_group = applications_by_salary_group.groupby(['Year', 'Salary Group'])['Applications'].sum().reset_index()\n",
    "\n",
    "# Splitting the data based on the Brexit cutoff for high and low salary groups\n",
    "pre_brexit_data_high = applications_by_salary_group[(applications_by_salary_group['Salary Group'] == 'High') & (applications_by_salary_group['Year'] <= 2020)]\n",
    "post_brexit_data_high = applications_by_salary_group[(applications_by_salary_group['Salary Group'] == 'High') & (applications_by_salary_group['Year'] > 2020)]\n",
    "\n",
    "pre_brexit_data_low = applications_by_salary_group[(applications_by_salary_group['Salary Group'] == 'Low') & (applications_by_salary_group['Year'] <= 2020)]\n",
    "post_brexit_data_low = applications_by_salary_group[(applications_by_salary_group['Salary Group'] == 'Low') & (applications_by_salary_group['Year'] > 2020)]\n",
    "\n",
    "# Fit the models for each segment\n",
    "model_pre_high = sm.OLS(pre_brexit_data_high['Applications'], sm.add_constant(pre_brexit_data_high['Year'])).fit()\n",
    "model_post_high = sm.OLS(post_brexit_data_high['Applications'], sm.add_constant(post_brexit_data_high['Year'])).fit()\n",
    "\n",
    "model_pre_low = sm.OLS(pre_brexit_data_low['Applications'], sm.add_constant(pre_brexit_data_low['Year'])).fit()\n",
    "model_post_low = sm.OLS(post_brexit_data_low['Applications'], sm.add_constant(post_brexit_data_low['Year'])).fit()\n",
    "\n",
    "# Summary tables for regression models\n",
    "summary_pre_high = model_pre_high.summary()\n",
    "summary_post_high = model_post_high.summary()\n",
    "summary_pre_low = model_pre_low.summary()\n",
    "summary_post_low = model_post_low.summary()\n",
    "\n",
    "# Output the summaries\n",
    "print(\"High Salary Group - Pre-Brexit Model Summary:\\n\", summary_pre_high)\n",
    "print(\"High Salary Group - Post-Brexit Model Summary:\\n\", summary_post_high)\n",
    "print(\"Low Salary Group - Pre-Brexit Model Summary:\\n\", summary_pre_low)\n",
    "print(\"Low Salary Group - Post-Brexit Model Summary:\\n\", summary_post_low)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
