{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff69a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "def setup_driver(download_dir):\n",
    "    \"\"\"Sets up the WebDriver for Chrome.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Disable PDF viewer to automatically download PDF files\n",
    "    options.add_experimental_option(\"prefs\", {\n",
    "        \"download.default_directory\": download_dir,\n",
    "        \"download.prompt_for_download\": False,  # Disable download prompt\n",
    "        \"plugins.always_open_pdf_externally\": True  # It will not show PDF directly in chrome\n",
    "    })\n",
    "    # Set up Chrome driver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "def download_pdfs_by_class(base_url, class_name, download_dir):\n",
    "    driver = setup_driver(download_dir)\n",
    "    driver.get(base_url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(1)  # Increase or decrease based on your internet speed\n",
    "\n",
    "    # Find all elements with the specified class and download the linked files\n",
    "    links = driver.find_elements(By.CLASS_NAME, class_name)\n",
    "    for link in links:\n",
    "        href = link.get_attribute('href')\n",
    "        if href and href.endswith('.pdf'):\n",
    "            # Open the link in a new tab\n",
    "            driver.execute_script(f\"window.open('{href}');\")\n",
    "            time.sleep(1)  # Adjust time for page load as necessary\n",
    "            # Switch back to the main window\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "# Base URL of the page containing the links\n",
    "base_url = 'https://info.lse.ac.uk/staff/divisions/Planning-Division/Table-of-Fees'\n",
    "# Class shared by PDF links\n",
    "class_name = 'sys_21'\n",
    "# Path to the download directory\n",
    "download_dir = 'Data\\TuitionFees'\n",
    "\n",
    "download_pdfs_by_class(base_url, class_name, download_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6def1702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/399378898.py:36: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/399378898.py:36: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
      "Skipping line 244: expected 5 fields, saw 8\n",
      "Skipping line 245: expected 5 fields, saw 8\n",
      "Skipping line 246: expected 5 fields, saw 8\n",
      "Skipping line 247: expected 5 fields, saw 8\n",
      "Skipping line 248: expected 5 fields, saw 8\n",
      "Skipping line 249: expected 5 fields, saw 8\n",
      "Skipping line 250: expected 5 fields, saw 9\n",
      "Skipping line 251: expected 5 fields, saw 9\n",
      "Skipping line 252: expected 5 fields, saw 9\n",
      "Skipping line 253: expected 5 fields, saw 9\n",
      "Skipping line 254: expected 5 fields, saw 9\n",
      "Skipping line 255: expected 5 fields, saw 9\n",
      "Skipping line 256: expected 5 fields, saw 9\n",
      "Skipping line 257: expected 5 fields, saw 9\n",
      "Skipping line 258: expected 5 fields, saw 9\n",
      "Skipping line 259: expected 5 fields, saw 9\n",
      "Skipping line 260: expected 5 fields, saw 9\n",
      "Skipping line 261: expected 5 fields, saw 9\n",
      "Skipping line 262: expected 5 fields, saw 9\n",
      "Skipping line 263: expected 5 fields, saw 9\n",
      "Skipping line 264: expected 5 fields, saw 9\n",
      "Skipping line 265: expected 5 fields, saw 9\n",
      "Skipping line 266: expected 5 fields, saw 9\n",
      "Skipping line 267: expected 5 fields, saw 9\n",
      "Skipping line 268: expected 5 fields, saw 9\n",
      "Skipping line 269: expected 5 fields, saw 9\n",
      "Skipping line 270: expected 5 fields, saw 9\n",
      "Skipping line 271: expected 5 fields, saw 9\n",
      "\n",
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/399378898.py:105: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  avg_col3 = pd.Series(col3_values).dropna().mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Department</th>\n",
       "      <th>Program</th>\n",
       "      <th>Home fees</th>\n",
       "      <th>Overseas fees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All 2020</td>\n",
       "      <td>UG Degree</td>\n",
       "      <td>9250</td>\n",
       "      <td>21570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>28080</td>\n",
       "      <td>28464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economic History</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14640</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Economics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>23179</td>\n",
       "      <td>24134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>European Institute</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19952</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Finance</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>28969</td>\n",
       "      <td>29185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gender Studies</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14640</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Geography And Environment</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14640</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Health Policy</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>18160</td>\n",
       "      <td>25768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>International Development</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14640</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>International Relations</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19452</td>\n",
       "      <td>23436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Law School</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>16656</td>\n",
       "      <td>24264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Management</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>28441</td>\n",
       "      <td>28703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mathematics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>21912</td>\n",
       "      <td>26184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Media And Communication</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>20616</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Philosophy Logic And Scientific Method</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>18624</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Psychological And Behavioural Science</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>17968</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>School of Public Policy</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>21987</td>\n",
       "      <td>24264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sociology</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14640</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Statistics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>24336</td>\n",
       "      <td>27376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Anthropology</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>17296</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>International History</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14640</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Media And Communication</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>20616</td>\n",
       "      <td>22608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>International..</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19470</td>\n",
       "      <td>23732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Department    Program  Home fees  \\\n",
       "0                                 All 2020  UG Degree       9250   \n",
       "1                               Accounting  PG Taught      28080   \n",
       "2                         Economic History  PG Taught      14640   \n",
       "3                                Economics  PG Taught      23179   \n",
       "4                       European Institute  PG Taught      19952   \n",
       "5                                  Finance  PG Taught      28969   \n",
       "6                           Gender Studies  PG Taught      14640   \n",
       "7                Geography And Environment  PG Taught      14640   \n",
       "8                            Health Policy  PG Taught      18160   \n",
       "9                International Development  PG Taught      14640   \n",
       "10                 International Relations  PG Taught      19452   \n",
       "11                              Law School  PG Taught      16656   \n",
       "12                              Management  PG Taught      28441   \n",
       "13                             Mathematics  PG Taught      21912   \n",
       "14                 Media And Communication  PG Taught      20616   \n",
       "15  Philosophy Logic And Scientific Method  PG Taught      18624   \n",
       "16   Psychological And Behavioural Science  PG Taught      17968   \n",
       "17                 School of Public Policy  PG Taught      21987   \n",
       "18                               Sociology  PG Taught      14640   \n",
       "19                              Statistics  PG Taught      24336   \n",
       "20                            Anthropology  PG Taught      17296   \n",
       "21                   International History  PG Taught      14640   \n",
       "22                 Media And Communication  PG Taught      20616   \n",
       "23                         International..  PG Taught      19470   \n",
       "\n",
       "    Overseas fees  \n",
       "0           21570  \n",
       "1           28464  \n",
       "2           22608  \n",
       "3           24134  \n",
       "4           22608  \n",
       "5           29185  \n",
       "6           22608  \n",
       "7           22608  \n",
       "8           25768  \n",
       "9           22608  \n",
       "10          23436  \n",
       "11          24264  \n",
       "12          28703  \n",
       "13          26184  \n",
       "14          22608  \n",
       "15          22608  \n",
       "16          22608  \n",
       "17          24264  \n",
       "18          22608  \n",
       "19          27376  \n",
       "20          22608  \n",
       "21          22608  \n",
       "22          22608  \n",
       "23          23732  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2020\n",
    "\n",
    "import pdfplumber\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, output_csv_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        all_tables = []\n",
    "        # Iterate through each page of the PDF\n",
    "        for page in pdf.pages:\n",
    "            # Extract tables from the current page\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                all_tables.extend(table)  # Add the rows of the table to all_tables list\n",
    "       \n",
    "        # Write all extracted tables to a CSV file\n",
    "        with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for row in all_tables:\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Specify the path to your PDF and the output CSV file\n",
    "pdf_path = 'Data/2020-Table-of-Fees-25Jun20.pdf'\n",
    "output_csv_path = 'Data/2020_Fees.csv'\n",
    "\n",
    "extract_tables_from_pdf(pdf_path, output_csv_path)\n",
    "\n",
    "\n",
    "# Load the CSV file containing the tuition fees and program data\n",
    "output_csv_path = 'Data/2020_Fees.csv'  \n",
    "csv_path = 'Data/Florian_Wirtz_eigentlich_noch_was_mit_der_v2.csv'  \n",
    "\n",
    "# Read the CSV files\n",
    "data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
    "data_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Combine 'Department' and 'Program' into a new column for unique combinations\n",
    "data_df['Dept_Program'] = data_df['Department'] + \" \" + data_df['Program']\n",
    "\n",
    "# Get unique combinations\n",
    "unique_dept_programs = data_df['Dept_Program'].unique()\n",
    "\n",
    "# Dictionary to store original to transformed mappings\n",
    "original_to_transformed = {}\n",
    "\n",
    "# Process only Master's programs and adjust department names\n",
    "processed_dept_programs = []\n",
    "for combo in unique_dept_programs:\n",
    "    if isinstance(combo, str):  # Check if the item is a string\n",
    "        if \"PG Taught\" in combo:\n",
    "            original_department = combo.replace(\" PG Taught\", \"\")\n",
    "            transformed_department = original_department\n",
    "            # Handle specific naming transformations\n",
    "            if \"And\" in transformed_department:\n",
    "                transformed_department = transformed_department.split(\"And\")[0].strip()\n",
    "            if transformed_department == \"International History\":\n",
    "                transformed_department = \"History\"\n",
    "            if transformed_department == \"European Institute\":\n",
    "                transformed_department = \"European\"\n",
    "            if transformed_department == \"Law School\" or transformed_department == \"Law\":\n",
    "                transformed_department = \"LLM\"\n",
    "            if transformed_department == \"Philosophy Logic\":\n",
    "                transformed_department = \"Philosophy\"\n",
    "            if transformed_department == \"School of Public Policy\":\n",
    "                transformed_department = \"Public Policy\"\n",
    "            if transformed_department == \"Gender Studies\":\n",
    "                transformed_department = \"Gender\"\n",
    "            if \"Psychological\" in transformed_department:\n",
    "                transformed_department = \"Psychology\"\n",
    "            \n",
    "            # Save mapping\n",
    "            original_to_transformed[transformed_department] = original_department\n",
    "            processed_dept_programs.append(transformed_department)\n",
    "\n",
    "# Create DataFrame from processed list\n",
    "processed_df = pd.DataFrame(processed_dept_programs, columns=['Department'])\n",
    "\n",
    "# Insert \"Program\" column with \"PG Taught\" as the value for all entries\n",
    "processed_df.insert(1, 'Program', 'PG Taught')\n",
    "\n",
    "def find_matching_data(dept_program):\n",
    "    matches = data_df_one[data_df_one.iloc[:, 0].str.contains(dept_program, na=False)]\n",
    "    if not matches.empty:\n",
    "        col2_values = []\n",
    "        col3_values = []\n",
    "        for _, row in matches.iterrows():\n",
    "            # Process and clean fee data for averaging\n",
    "            try:\n",
    "                cleaned_value_col2 = float(str(row[1]).replace('£', '').replace(',', ''))\n",
    "                if cleaned_value_col2.is_integer():\n",
    "                    col2_values.append(int(cleaned_value_col2))\n",
    "            except ValueError:\n",
    "                col2_values.append(pd.NA)\n",
    "            try:\n",
    "                cleaned_value_col3 = float(str(row[2]).replace('£', '').replace(',', ''))\n",
    "                if cleaned_value_col3.is_integer():\n",
    "                    col3_values.append(int(cleaned_value_col3))\n",
    "            except ValueError:\n",
    "                col3_values.append(pd.NA)\n",
    "        \n",
    "        # Calculate averages while ignoring N/A values\n",
    "        avg_col2 = pd.Series(col2_values).dropna().mean()\n",
    "        avg_col3 = pd.Series(col3_values).dropna().mean()\n",
    "        return pd.Series([avg_col2, avg_col3])\n",
    "    return pd.Series([pd.NA, pd.NA])\n",
    "\n",
    "# Apply the function to find and average matching tuition fees\n",
    "processed_df[['Home fees', 'Overseas fees']] = processed_df['Department'].apply(find_matching_data)\n",
    "\n",
    "# Remove rows where either column contains NA or NaN values\n",
    "processed_df.dropna(subset=['Home fees', 'Overseas fees'], inplace=True)\n",
    "\n",
    "# Ensure all remaining values are integers\n",
    "processed_df['Home fees'] = processed_df['Home fees'].astype(int)\n",
    "processed_df['Overseas fees'] = processed_df['Overseas fees'].astype(int)\n",
    "\n",
    "# Revert department names to original values\n",
    "processed_df['Department'] = processed_df['Department'].map(original_to_transformed)\n",
    "\n",
    "# Display the final DataFrame\n",
    "processed_df\n",
    "\n",
    "# Find the row index for the specific phrase and extract the fees and year\n",
    "idx = data_df_one.index[data_df_one.iloc[:, 0].str.contains(\"Students commencing their degree in\" or \"New Entrants\", na=False)].tolist()\n",
    "if idx:\n",
    "    target_idx = idx[0]  # Assume the first occurrence\n",
    "    if \"Undergraduate\" in data_df_one.iloc[target_idx - 1, 0]:\n",
    "        year_match = re.search(r'\\b(\\d{4})\\b', data_df_one.iloc[target_idx, 0])\n",
    "        year = year_match.group(0) if year_match else \"Unknown\"\n",
    "        home_fee_match = re.search(r'£(\\d{4})', data_df_one.iloc[target_idx, 1].replace(',', ''))\n",
    "        overseas_fee_match = re.search(r'£(\\d{5})', data_df_one.iloc[target_idx, 2].replace(',', ''))\n",
    "        home_fee = int(home_fee_match.group(1)) if home_fee_match else None\n",
    "        overseas_fee = int(overseas_fee_match.group(1)) if overseas_fee_match else None\n",
    "\n",
    "# Add the extracted row to the top of the processed_df DataFrame\n",
    "additional_row = pd.DataFrame({\n",
    "    'Department': [f\"All {year}\"],\n",
    "    'Program': [\"UG Degree\"],\n",
    "    'Home fees': [home_fee],\n",
    "    'Overseas fees': [overseas_fee]\n",
    "})\n",
    "\n",
    "# Append the additional row to the processed_df DataFrame\n",
    "processed_df = pd.concat([additional_row, processed_df]).reset_index(drop=True)\n",
    "\n",
    "# Display the final DataFrame including the new row\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69df80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e166c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed73d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eca0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6823006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/980795836.py:31: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/980795836.py:31: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/980795836.py:100: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  avg_col3 = pd.Series(col3_values).dropna().mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Department</th>\n",
       "      <th>Program</th>\n",
       "      <th>Home fees</th>\n",
       "      <th>Overseas fees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All 2018</td>\n",
       "      <td>UG Degree</td>\n",
       "      <td>9250</td>\n",
       "      <td>19152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>25080</td>\n",
       "      <td>25344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economic History</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13536</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Economics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>20848</td>\n",
       "      <td>21796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>European Institute</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19936</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Finance</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>26599</td>\n",
       "      <td>26797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gender Studies</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13536</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Geography And Environment</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13536</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Health Policy</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>16392</td>\n",
       "      <td>23448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>International Development</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>17480</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>International Relations</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>17988</td>\n",
       "      <td>21672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Law School</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>15384</td>\n",
       "      <td>22440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Management</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>23670</td>\n",
       "      <td>23756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mathematics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>20256</td>\n",
       "      <td>24204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Media And Communication</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19062</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Philosophy Logic And Scientific Method</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>17220</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Psychological And Behavioural Science</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>15992</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>School of Public Policy</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>20363</td>\n",
       "      <td>20954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Social Policy</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>18048</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sociology</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13536</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Statistics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>20256</td>\n",
       "      <td>24204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Anthropology</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>15992</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>International History</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13536</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Media And Communication</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19062</td>\n",
       "      <td>20904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>International..</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>18922</td>\n",
       "      <td>22326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Department    Program  Home fees  \\\n",
       "0                                 All 2018  UG Degree       9250   \n",
       "1                               Accounting  PG Taught      25080   \n",
       "2                         Economic History  PG Taught      13536   \n",
       "3                                Economics  PG Taught      20848   \n",
       "4                       European Institute  PG Taught      19936   \n",
       "5                                  Finance  PG Taught      26599   \n",
       "6                           Gender Studies  PG Taught      13536   \n",
       "7                Geography And Environment  PG Taught      13536   \n",
       "8                            Health Policy  PG Taught      16392   \n",
       "9                International Development  PG Taught      17480   \n",
       "10                 International Relations  PG Taught      17988   \n",
       "11                              Law School  PG Taught      15384   \n",
       "12                              Management  PG Taught      23670   \n",
       "13                             Mathematics  PG Taught      20256   \n",
       "14                 Media And Communication  PG Taught      19062   \n",
       "15  Philosophy Logic And Scientific Method  PG Taught      17220   \n",
       "16   Psychological And Behavioural Science  PG Taught      15992   \n",
       "17                 School of Public Policy  PG Taught      20363   \n",
       "18                           Social Policy  PG Taught      18048   \n",
       "19                               Sociology  PG Taught      13536   \n",
       "20                              Statistics  PG Taught      20256   \n",
       "21                            Anthropology  PG Taught      15992   \n",
       "22                   International History  PG Taught      13536   \n",
       "23                 Media And Communication  PG Taught      19062   \n",
       "24                         International..  PG Taught      18922   \n",
       "\n",
       "    Overseas fees  \n",
       "0           19152  \n",
       "1           25344  \n",
       "2           20904  \n",
       "3           21796  \n",
       "4           20904  \n",
       "5           26797  \n",
       "6           20904  \n",
       "7           20904  \n",
       "8           23448  \n",
       "9           20904  \n",
       "10          21672  \n",
       "11          22440  \n",
       "12          23756  \n",
       "13          24204  \n",
       "14          20904  \n",
       "15          20904  \n",
       "16          20904  \n",
       "17          20954  \n",
       "18          20904  \n",
       "19          20904  \n",
       "20          24204  \n",
       "21          20904  \n",
       "22          20904  \n",
       "23          20904  \n",
       "24          22326  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2018\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, output_csv_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        all_tables = []\n",
    "        # Iterate through each page of the PDF\n",
    "        for page in pdf.pages:\n",
    "            # Extract tables from the current page\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                all_tables.extend(table)  # Add the rows of the table to all_tables list\n",
    "       \n",
    "        # Write all extracted tables to a CSV file\n",
    "        with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for row in all_tables:\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Specify the path to your PDF and the output CSV file\n",
    "pdf_path = 'Data/2018-19-Fees-Table.pdf'\n",
    "output_csv_path = 'Data/2018_Fees.csv'\n",
    "\n",
    "extract_tables_from_pdf(pdf_path, output_csv_path)\n",
    "\n",
    "\n",
    "# Load the CSV file containing the tuition fees and program data\n",
    "output_csv_path = 'Data/2018_Fees.csv'  \n",
    "csv_path = 'Data/Florian_Wirtz_eigentlich_noch_was_mit_der_v2.csv'  \n",
    "\n",
    "# Read the CSV files\n",
    "data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
    "data_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Combine 'Department' and 'Program' into a new column for unique combinations\n",
    "data_df['Dept_Program'] = data_df['Department'] + \" \" + data_df['Program']\n",
    "\n",
    "# Get unique combinations\n",
    "unique_dept_programs = data_df['Dept_Program'].unique()\n",
    "\n",
    "# Dictionary to store original to transformed mappings\n",
    "original_to_transformed = {}\n",
    "\n",
    "# Process only Master's programs and adjust department names\n",
    "processed_dept_programs = []\n",
    "for combo in unique_dept_programs:\n",
    "    if isinstance(combo, str):  # Check if the item is a string\n",
    "        if \"PG Taught\" in combo:\n",
    "            original_department = combo.replace(\" PG Taught\", \"\")\n",
    "            transformed_department = original_department\n",
    "            # Handle specific naming transformations\n",
    "            if \"And\" in transformed_department:\n",
    "                transformed_department = transformed_department.split(\"And\")[0].strip()\n",
    "            if transformed_department == \"International History\":\n",
    "                transformed_department = \"History\"\n",
    "            if transformed_department == \"European Institute\":\n",
    "                transformed_department = \"European\"\n",
    "            if transformed_department == \"Law School\" or transformed_department == \"Law\":\n",
    "                transformed_department = \"LLM\"\n",
    "            if transformed_department == \"Philosophy Logic\":\n",
    "                transformed_department = \"Philosophy\"\n",
    "            if transformed_department == \"School of Public Policy\":\n",
    "                transformed_department = \"Public Policy\"\n",
    "            if transformed_department == \"Gender Studies\":\n",
    "                transformed_department = \"Gender\"\n",
    "            if \"Psychological\" in transformed_department:\n",
    "                transformed_department = \"Psychology\"\n",
    "            \n",
    "            # Save mapping\n",
    "            original_to_transformed[transformed_department] = original_department\n",
    "            processed_dept_programs.append(transformed_department)\n",
    "\n",
    "# Create DataFrame from processed list\n",
    "processed_df = pd.DataFrame(processed_dept_programs, columns=['Department'])\n",
    "\n",
    "# Insert \"Program\" column with \"PG Taught\" as the value for all entries\n",
    "processed_df.insert(1, 'Program', 'PG Taught')\n",
    "\n",
    "def find_matching_data(dept_program):\n",
    "    matches = data_df_one[data_df_one.iloc[:, 0].str.contains(dept_program, na=False)]\n",
    "    if not matches.empty:\n",
    "        col2_values = []\n",
    "        col3_values = []\n",
    "        for _, row in matches.iterrows():\n",
    "            # Process and clean fee data for averaging\n",
    "            try:\n",
    "                cleaned_value_col2 = float(str(row[1]).replace('£', '').replace(',', ''))\n",
    "                if cleaned_value_col2.is_integer():\n",
    "                    col2_values.append(int(cleaned_value_col2))\n",
    "            except ValueError:\n",
    "                col2_values.append(pd.NA)\n",
    "            try:\n",
    "                cleaned_value_col3 = float(str(row[2]).replace('£', '').replace(',', ''))\n",
    "                if cleaned_value_col3.is_integer():\n",
    "                    col3_values.append(int(cleaned_value_col3))\n",
    "            except ValueError:\n",
    "                col3_values.append(pd.NA)\n",
    "        \n",
    "        # Calculate averages while ignoring N/A values\n",
    "        avg_col2 = pd.Series(col2_values).dropna().mean()\n",
    "        avg_col3 = pd.Series(col3_values).dropna().mean()\n",
    "        return pd.Series([avg_col2, avg_col3])\n",
    "    return pd.Series([pd.NA, pd.NA])\n",
    "\n",
    "# Apply the function to find and average matching tuition fees\n",
    "processed_df[['Home fees', 'Overseas fees']] = processed_df['Department'].apply(find_matching_data)\n",
    "\n",
    "# Remove rows where either column contains NA or NaN values\n",
    "processed_df.dropna(subset=['Home fees', 'Overseas fees'], inplace=True)\n",
    "\n",
    "# Ensure all remaining values are integers\n",
    "processed_df['Home fees'] = processed_df['Home fees'].astype(int)\n",
    "processed_df['Overseas fees'] = processed_df['Overseas fees'].astype(int)\n",
    "\n",
    "# Revert department names to original values\n",
    "processed_df['Department'] = processed_df['Department'].map(original_to_transformed)\n",
    "\n",
    "# Display the final DataFrame\n",
    "processed_df\n",
    "\n",
    "# Find the row index for the specific phrase and extract the fees and year\n",
    "idx = data_df_one.index[data_df_one.iloc[:, 0].str.contains(\"Students commencing their degree in\" or \"New Entrants\", na=False)].tolist()\n",
    "if idx:\n",
    "    target_idx = idx[0]  # Assume the first occurrence\n",
    "    if \"Undergraduate\" in data_df_one.iloc[target_idx - 1, 0]:\n",
    "        year_match = re.search(r'\\b(\\d{4})\\b', data_df_one.iloc[target_idx, 0])\n",
    "        year = year_match.group(0) if year_match else \"Unknown\"\n",
    "        home_fee_match = re.search(r'£(\\d{4})', data_df_one.iloc[target_idx, 1].replace(',', ''))\n",
    "        overseas_fee_match = re.search(r'£(\\d{5})', data_df_one.iloc[target_idx, 2].replace(',', ''))\n",
    "        home_fee = int(home_fee_match.group(1)) if home_fee_match else None\n",
    "        overseas_fee = int(overseas_fee_match.group(1)) if overseas_fee_match else None\n",
    "\n",
    "# Add the extracted row to the top of the processed_df DataFrame\n",
    "additional_row = pd.DataFrame({\n",
    "    'Department': [f\"All {year}\"],\n",
    "    'Program': [\"UG Degree\"],\n",
    "    'Home fees': [home_fee],\n",
    "    'Overseas fees': [overseas_fee]\n",
    "})\n",
    "\n",
    "# Append the additional row to the processed_df DataFrame\n",
    "processed_df = pd.concat([additional_row, processed_df]).reset_index(drop=True)\n",
    "\n",
    "# Display the final DataFrame including the new row\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346bd336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25be78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b045a997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/3268269658.py:31: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/3268269658.py:31: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
      "Skipping line 234: expected 5 fields, saw 8\n",
      "Skipping line 235: expected 5 fields, saw 8\n",
      "Skipping line 236: expected 5 fields, saw 8\n",
      "Skipping line 237: expected 5 fields, saw 8\n",
      "Skipping line 238: expected 5 fields, saw 8\n",
      "Skipping line 239: expected 5 fields, saw 8\n",
      "Skipping line 240: expected 5 fields, saw 9\n",
      "Skipping line 241: expected 5 fields, saw 9\n",
      "Skipping line 242: expected 5 fields, saw 9\n",
      "Skipping line 243: expected 5 fields, saw 9\n",
      "Skipping line 244: expected 5 fields, saw 9\n",
      "Skipping line 245: expected 5 fields, saw 9\n",
      "Skipping line 246: expected 5 fields, saw 9\n",
      "Skipping line 247: expected 5 fields, saw 9\n",
      "Skipping line 248: expected 5 fields, saw 9\n",
      "Skipping line 249: expected 5 fields, saw 9\n",
      "Skipping line 250: expected 5 fields, saw 9\n",
      "Skipping line 251: expected 5 fields, saw 9\n",
      "Skipping line 252: expected 5 fields, saw 9\n",
      "Skipping line 253: expected 5 fields, saw 9\n",
      "Skipping line 254: expected 5 fields, saw 9\n",
      "Skipping line 255: expected 5 fields, saw 9\n",
      "Skipping line 256: expected 5 fields, saw 9\n",
      "Skipping line 257: expected 5 fields, saw 9\n",
      "Skipping line 258: expected 5 fields, saw 9\n",
      "Skipping line 259: expected 5 fields, saw 9\n",
      "Skipping line 260: expected 5 fields, saw 9\n",
      "Skipping line 261: expected 5 fields, saw 9\n",
      "\n",
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/3268269658.py:100: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  avg_col3 = pd.Series(col3_values).dropna().mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Department</th>\n",
       "      <th>Program</th>\n",
       "      <th>Home fees</th>\n",
       "      <th>Overseas fees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All 2019</td>\n",
       "      <td>UG Degree</td>\n",
       "      <td>9250</td>\n",
       "      <td>19920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>26082</td>\n",
       "      <td>26358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economic History</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14088</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Economics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>21917</td>\n",
       "      <td>22870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>European Institute</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19192</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Finance</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>27663</td>\n",
       "      <td>27870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gender Studies</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14088</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Geography And Environment</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14088</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Health Policy</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>17512</td>\n",
       "      <td>24832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>International Development</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14088</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>International Relations</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>18708</td>\n",
       "      <td>22536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Law School</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>16008</td>\n",
       "      <td>23328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Management</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>24575</td>\n",
       "      <td>24826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mathematics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>21072</td>\n",
       "      <td>25176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Media And Communication</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19830</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Philosophy Logic And Scientific Method</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>17916</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Psychological And Behavioural Science</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>17280</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>School of Public Policy</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>22294</td>\n",
       "      <td>23570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sociology</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14088</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Statistics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>23400</td>\n",
       "      <td>26320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Anthropology</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>16640</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>International History</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14088</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Media And Communication</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19830</td>\n",
       "      <td>21744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>International..</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19194</td>\n",
       "      <td>22994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Department    Program  Home fees  \\\n",
       "0                                 All 2019  UG Degree       9250   \n",
       "1                               Accounting  PG Taught      26082   \n",
       "2                         Economic History  PG Taught      14088   \n",
       "3                                Economics  PG Taught      21917   \n",
       "4                       European Institute  PG Taught      19192   \n",
       "5                                  Finance  PG Taught      27663   \n",
       "6                           Gender Studies  PG Taught      14088   \n",
       "7                Geography And Environment  PG Taught      14088   \n",
       "8                            Health Policy  PG Taught      17512   \n",
       "9                International Development  PG Taught      14088   \n",
       "10                 International Relations  PG Taught      18708   \n",
       "11                              Law School  PG Taught      16008   \n",
       "12                              Management  PG Taught      24575   \n",
       "13                             Mathematics  PG Taught      21072   \n",
       "14                 Media And Communication  PG Taught      19830   \n",
       "15  Philosophy Logic And Scientific Method  PG Taught      17916   \n",
       "16   Psychological And Behavioural Science  PG Taught      17280   \n",
       "17                 School of Public Policy  PG Taught      22294   \n",
       "18                               Sociology  PG Taught      14088   \n",
       "19                              Statistics  PG Taught      23400   \n",
       "20                            Anthropology  PG Taught      16640   \n",
       "21                   International History  PG Taught      14088   \n",
       "22                 Media And Communication  PG Taught      19830   \n",
       "23                         International..  PG Taught      19194   \n",
       "\n",
       "    Overseas fees  \n",
       "0           19920  \n",
       "1           26358  \n",
       "2           21744  \n",
       "3           22870  \n",
       "4           21744  \n",
       "5           27870  \n",
       "6           21744  \n",
       "7           21744  \n",
       "8           24832  \n",
       "9           21744  \n",
       "10          22536  \n",
       "11          23328  \n",
       "12          24826  \n",
       "13          25176  \n",
       "14          21744  \n",
       "15          21744  \n",
       "16          21744  \n",
       "17          23570  \n",
       "18          21744  \n",
       "19          26320  \n",
       "20          21744  \n",
       "21          21744  \n",
       "22          21744  \n",
       "23          22994  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2019\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, output_csv_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        all_tables = []\n",
    "        # Iterate through each page of the PDF\n",
    "        for page in pdf.pages:\n",
    "            # Extract tables from the current page\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                all_tables.extend(table)  # Add the rows of the table to all_tables list\n",
    "       \n",
    "        # Write all extracted tables to a CSV file\n",
    "        with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for row in all_tables:\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Specify the path to your PDF and the output CSV file\n",
    "pdf_path = 'Data/2019-Table-of-Fees.pdf'\n",
    "output_csv_path = 'Data/2019_Fees.csv'\n",
    "\n",
    "extract_tables_from_pdf(pdf_path, output_csv_path)\n",
    "\n",
    "\n",
    "# Load the CSV file containing the tuition fees and program data\n",
    "output_csv_path = 'Data/2019_Fees.csv'  \n",
    "csv_path = 'Data/Florian_Wirtz_eigentlich_noch_was_mit_der_v2.csv'  \n",
    "\n",
    "# Read the CSV files\n",
    "data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
    "data_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Combine 'Department' and 'Program' into a new column for unique combinations\n",
    "data_df['Dept_Program'] = data_df['Department'] + \" \" + data_df['Program']\n",
    "\n",
    "# Get unique combinations\n",
    "unique_dept_programs = data_df['Dept_Program'].unique()\n",
    "\n",
    "# Dictionary to store original to transformed mappings\n",
    "original_to_transformed = {}\n",
    "\n",
    "# Process only Master's programs and adjust department names\n",
    "processed_dept_programs = []\n",
    "for combo in unique_dept_programs:\n",
    "    if isinstance(combo, str):  # Check if the item is a string\n",
    "        if \"PG Taught\" in combo:\n",
    "            original_department = combo.replace(\" PG Taught\", \"\")\n",
    "            transformed_department = original_department\n",
    "            # Handle specific naming transformations\n",
    "            if \"And\" in transformed_department:\n",
    "                transformed_department = transformed_department.split(\"And\")[0].strip()\n",
    "            if transformed_department == \"International History\":\n",
    "                transformed_department = \"History\"\n",
    "            if transformed_department == \"European Institute\":\n",
    "                transformed_department = \"European\"\n",
    "            if transformed_department == \"Law School\" or transformed_department == \"Law\":\n",
    "                transformed_department = \"LLM\"\n",
    "            if transformed_department == \"Philosophy Logic\":\n",
    "                transformed_department = \"Philosophy\"\n",
    "            if transformed_department == \"School of Public Policy\":\n",
    "                transformed_department = \"Public Policy\"\n",
    "            if transformed_department == \"Gender Studies\":\n",
    "                transformed_department = \"Gender\"\n",
    "            if \"Psychological\" in transformed_department:\n",
    "                transformed_department = \"Psychology\"\n",
    "            \n",
    "            # Save mapping\n",
    "            original_to_transformed[transformed_department] = original_department\n",
    "            processed_dept_programs.append(transformed_department)\n",
    "\n",
    "# Create DataFrame from processed list\n",
    "processed_df = pd.DataFrame(processed_dept_programs, columns=['Department'])\n",
    "\n",
    "# Insert \"Program\" column with \"PG Taught\" as the value for all entries\n",
    "processed_df.insert(1, 'Program', 'PG Taught')\n",
    "\n",
    "def find_matching_data(dept_program):\n",
    "    matches = data_df_one[data_df_one.iloc[:, 0].str.contains(dept_program, na=False)]\n",
    "    if not matches.empty:\n",
    "        col2_values = []\n",
    "        col3_values = []\n",
    "        for _, row in matches.iterrows():\n",
    "            # Process and clean fee data for averaging\n",
    "            try:\n",
    "                cleaned_value_col2 = float(str(row[1]).replace('£', '').replace(',', ''))\n",
    "                if cleaned_value_col2.is_integer():\n",
    "                    col2_values.append(int(cleaned_value_col2))\n",
    "            except ValueError:\n",
    "                col2_values.append(pd.NA)\n",
    "            try:\n",
    "                cleaned_value_col3 = float(str(row[2]).replace('£', '').replace(',', ''))\n",
    "                if cleaned_value_col3.is_integer():\n",
    "                    col3_values.append(int(cleaned_value_col3))\n",
    "            except ValueError:\n",
    "                col3_values.append(pd.NA)\n",
    "        \n",
    "        # Calculate averages while ignoring N/A values\n",
    "        avg_col2 = pd.Series(col2_values).dropna().mean()\n",
    "        avg_col3 = pd.Series(col3_values).dropna().mean()\n",
    "        return pd.Series([avg_col2, avg_col3])\n",
    "    return pd.Series([pd.NA, pd.NA])\n",
    "\n",
    "# Apply the function to find and average matching tuition fees\n",
    "processed_df[['Home fees', 'Overseas fees']] = processed_df['Department'].apply(find_matching_data)\n",
    "\n",
    "# Remove rows where either column contains NA or NaN values\n",
    "processed_df.dropna(subset=['Home fees', 'Overseas fees'], inplace=True)\n",
    "\n",
    "# Ensure all remaining values are integers\n",
    "processed_df['Home fees'] = processed_df['Home fees'].astype(int)\n",
    "processed_df['Overseas fees'] = processed_df['Overseas fees'].astype(int)\n",
    "\n",
    "# Revert department names to original values\n",
    "processed_df['Department'] = processed_df['Department'].map(original_to_transformed)\n",
    "\n",
    "# Display the final DataFrame\n",
    "processed_df\n",
    "\n",
    "# Find the row index for the specific phrase and extract the fees and year\n",
    "idx = data_df_one.index[data_df_one.iloc[:, 0].str.contains(\"Students commencing their degree in\" or \"New Entrants\", na=False)].tolist()\n",
    "if idx:\n",
    "    target_idx = idx[0]  # Assume the first occurrence\n",
    "    if \"Undergraduate\" in data_df_one.iloc[target_idx - 1, 0]:\n",
    "        year_match = re.search(r'\\b(\\d{4})\\b', data_df_one.iloc[target_idx, 0])\n",
    "        year = year_match.group(0) if year_match else \"Unknown\"\n",
    "        home_fee_match = re.search(r'£(\\d{4})', data_df_one.iloc[target_idx, 1].replace(',', ''))\n",
    "        overseas_fee_match = re.search(r'£(\\d{5})', data_df_one.iloc[target_idx, 2].replace(',', ''))\n",
    "        home_fee = int(home_fee_match.group(1)) if home_fee_match else None\n",
    "        overseas_fee = int(overseas_fee_match.group(1)) if overseas_fee_match else None\n",
    "\n",
    "# Add the extracted row to the top of the processed_df DataFrame\n",
    "additional_row = pd.DataFrame({\n",
    "    'Department': [f\"All {year}\"],\n",
    "    'Program': [\"UG Degree\"],\n",
    "    'Home fees': [home_fee],\n",
    "    'Overseas fees': [overseas_fee]\n",
    "})\n",
    "\n",
    "# Append the additional row to the processed_df DataFrame\n",
    "processed_df = pd.concat([additional_row, processed_df]).reset_index(drop=True)\n",
    "\n",
    "# Display the final DataFrame including the new row\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5146e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673cbf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff608253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0689754d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/3885057974.py:31: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/3885057974.py:31: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Department</th>\n",
       "      <th>Program</th>\n",
       "      <th>Home fees</th>\n",
       "      <th>Overseas fees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All 2019</td>\n",
       "      <td>UG Degree</td>\n",
       "      <td>9250</td>\n",
       "      <td>19920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>24120</td>\n",
       "      <td>24372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economic History</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13008</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Economics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19607</td>\n",
       "      <td>21054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>European Institute</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>17587</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Finance</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>25578</td>\n",
       "      <td>25767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gender Studies</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13008</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Geography And Environment</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13008</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Government</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13208</td>\n",
       "      <td>20312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Health Policy</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>15792</td>\n",
       "      <td>22584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>International Development</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>16800</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>International Relations</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>17292</td>\n",
       "      <td>20844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Law School</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14784</td>\n",
       "      <td>21576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Management</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>22479</td>\n",
       "      <td>22491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mathematics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>19476</td>\n",
       "      <td>23280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Media And Communication</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>18336</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Philosophy Logic And Scientific Method</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>16560</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Psychological And Behavioural Science</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>15376</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>School of Public Policy</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>21180</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Social Policy</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13538</td>\n",
       "      <td>20178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sociology</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13008</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Statistics</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>17320</td>\n",
       "      <td>22224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Anthropology</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>14784</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>International History</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>13008</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Media And Communication</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>18336</td>\n",
       "      <td>20112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>International..</td>\n",
       "      <td>PG Taught</td>\n",
       "      <td>18409</td>\n",
       "      <td>21684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Department    Program  Home fees  \\\n",
       "0                                 All 2019  UG Degree       9250   \n",
       "1                               Accounting  PG Taught      24120   \n",
       "2                         Economic History  PG Taught      13008   \n",
       "3                                Economics  PG Taught      19607   \n",
       "4                       European Institute  PG Taught      17587   \n",
       "5                                  Finance  PG Taught      25578   \n",
       "6                           Gender Studies  PG Taught      13008   \n",
       "7                Geography And Environment  PG Taught      13008   \n",
       "8                               Government  PG Taught      13208   \n",
       "9                            Health Policy  PG Taught      15792   \n",
       "10               International Development  PG Taught      16800   \n",
       "11                 International Relations  PG Taught      17292   \n",
       "12                              Law School  PG Taught      14784   \n",
       "13                              Management  PG Taught      22479   \n",
       "14                             Mathematics  PG Taught      19476   \n",
       "15                 Media And Communication  PG Taught      18336   \n",
       "16  Philosophy Logic And Scientific Method  PG Taught      16560   \n",
       "17   Psychological And Behavioural Science  PG Taught      15376   \n",
       "18                 School of Public Policy  PG Taught      21180   \n",
       "19                           Social Policy  PG Taught      13538   \n",
       "20                               Sociology  PG Taught      13008   \n",
       "21                              Statistics  PG Taught      17320   \n",
       "22                            Anthropology  PG Taught      14784   \n",
       "23                   International History  PG Taught      13008   \n",
       "24                 Media And Communication  PG Taught      18336   \n",
       "25                         International..  PG Taught      18409   \n",
       "\n",
       "    Overseas fees  \n",
       "0           19920  \n",
       "1           24372  \n",
       "2           20112  \n",
       "3           21054  \n",
       "4           20112  \n",
       "5           25767  \n",
       "6           20112  \n",
       "7           20112  \n",
       "8           20312  \n",
       "9           22584  \n",
       "10          20112  \n",
       "11          20844  \n",
       "12          21576  \n",
       "13          22491  \n",
       "14          23280  \n",
       "15          20112  \n",
       "16          20112  \n",
       "17          20112  \n",
       "18          20112  \n",
       "19          20178  \n",
       "20          20112  \n",
       "21          22224  \n",
       "22          20112  \n",
       "23          20112  \n",
       "24          20112  \n",
       "25          21684  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2017\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, output_csv_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        all_tables = []\n",
    "        # Iterate through each page of the PDF\n",
    "        for page in pdf.pages:\n",
    "            # Extract tables from the current page\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                all_tables.extend(table)  # Add the rows of the table to all_tables list\n",
    "       \n",
    "        # Write all extracted tables to a CSV file\n",
    "        with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for row in all_tables:\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Specify the path to your PDF and the output CSV file\n",
    "pdf_path = 'Data/2017-18-Fees-Table.pdf'\n",
    "output_csv_path = 'Data/2017_Fees.csv'\n",
    "\n",
    "extract_tables_from_pdf(pdf_path, output_csv_path)\n",
    "\n",
    "\n",
    "# Load the CSV file containing the tuition fees and program data\n",
    "output_csv_path = 'Data/2017_Fees.csv'  \n",
    "csv_path = 'Data/Florian_Wirtz_eigentlich_noch_was_mit_der_v2.csv'  \n",
    "\n",
    "# Read the CSV files\n",
    "data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
    "data_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Combine 'Department' and 'Program' into a new column for unique combinations\n",
    "data_df['Dept_Program'] = data_df['Department'] + \" \" + data_df['Program']\n",
    "\n",
    "# Get unique combinations\n",
    "unique_dept_programs = data_df['Dept_Program'].unique()\n",
    "\n",
    "# Dictionary to store original to transformed mappings\n",
    "original_to_transformed = {}\n",
    "\n",
    "# Process only Master's programs and adjust department names\n",
    "processed_dept_programs = []\n",
    "for combo in unique_dept_programs:\n",
    "    if isinstance(combo, str):  # Check if the item is a string\n",
    "        if \"PG Taught\" in combo:\n",
    "            original_department = combo.replace(\" PG Taught\", \"\")\n",
    "            transformed_department = original_department\n",
    "            # Handle specific naming transformations\n",
    "            if \"And\" in transformed_department:\n",
    "                transformed_department = transformed_department.split(\"And\")[0].strip()\n",
    "            if transformed_department == \"International History\":\n",
    "                transformed_department = \"History\"\n",
    "            if transformed_department == \"European Institute\":\n",
    "                transformed_department = \"European\"\n",
    "            if transformed_department == \"Law School\" or transformed_department == \"Law\":\n",
    "                transformed_department = \"LLM\"\n",
    "            if transformed_department == \"Philosophy Logic\":\n",
    "                transformed_department = \"Philosophy\"\n",
    "            if transformed_department == \"School of Public Policy\":\n",
    "                transformed_department = \"Public Policy\"\n",
    "            if transformed_department == \"Gender Studies\":\n",
    "                transformed_department = \"Gender\"\n",
    "            if \"Psychological\" in transformed_department:\n",
    "                transformed_department = \"Psychology\"\n",
    "            \n",
    "            # Save mapping\n",
    "            original_to_transformed[transformed_department] = original_department\n",
    "            processed_dept_programs.append(transformed_department)\n",
    "\n",
    "# Create DataFrame from processed list\n",
    "processed_df = pd.DataFrame(processed_dept_programs, columns=['Department'])\n",
    "\n",
    "# Insert \"Program\" column with \"PG Taught\" as the value for all entries\n",
    "processed_df.insert(1, 'Program', 'PG Taught')\n",
    "\n",
    "def find_matching_data(dept_program):\n",
    "    matches = data_df_one[data_df_one.iloc[:, 0].str.contains(dept_program, na=False)]\n",
    "    if not matches.empty:\n",
    "        col2_values = []\n",
    "        col3_values = []\n",
    "        for _, row in matches.iterrows():\n",
    "            # Process and clean fee data for averaging\n",
    "            try:\n",
    "                cleaned_value_col2 = float(str(row[1]).replace('£', '').replace(',', ''))\n",
    "                if cleaned_value_col2.is_integer():\n",
    "                    col2_values.append(int(cleaned_value_col2))\n",
    "            except ValueError:\n",
    "                col2_values.append(pd.NA)\n",
    "            try:\n",
    "                cleaned_value_col3 = float(str(row[2]).replace('£', '').replace(',', ''))\n",
    "                if cleaned_value_col3.is_integer():\n",
    "                    col3_values.append(int(cleaned_value_col3))\n",
    "            except ValueError:\n",
    "                col3_values.append(pd.NA)\n",
    "        \n",
    "        # Calculate averages while ignoring N/A values\n",
    "        avg_col2 = pd.Series(col2_values).dropna().mean()\n",
    "        avg_col3 = pd.Series(col3_values).dropna().mean()\n",
    "        return pd.Series([avg_col2, avg_col3])\n",
    "    return pd.Series([pd.NA, pd.NA])\n",
    "\n",
    "# Apply the function to find and average matching tuition fees\n",
    "processed_df[['Home fees', 'Overseas fees']] = processed_df['Department'].apply(find_matching_data)\n",
    "\n",
    "# Remove rows where either column contains NA or NaN values\n",
    "processed_df.dropna(subset=['Home fees', 'Overseas fees'], inplace=True)\n",
    "\n",
    "# Ensure all remaining values are integers\n",
    "processed_df['Home fees'] = processed_df['Home fees'].astype(int)\n",
    "processed_df['Overseas fees'] = processed_df['Overseas fees'].astype(int)\n",
    "\n",
    "# Revert department names to original values\n",
    "processed_df['Department'] = processed_df['Department'].map(original_to_transformed)\n",
    "\n",
    "# Display the final DataFrame\n",
    "processed_df\n",
    "\n",
    "# Find the row index for the specific phrase and extract the fees and year\n",
    "idx = data_df_one.index[data_df_one.iloc[:, 0].str.contains(\"Students commencing their degree in\" or \"New Entrants\", na=False)].tolist()\n",
    "if idx:\n",
    "    target_idx = idx[0]  # Assume the first occurrence\n",
    "    if \"Undergraduate\" in data_df_one.iloc[target_idx - 1, 0]:\n",
    "        year_match = re.search(r'\\b(\\d{4})\\b', data_df_one.iloc[target_idx, 0])\n",
    "        year = year_match.group(0) if year_match else \"Unknown\"\n",
    "        home_fee_match = re.search(r'£(\\d{4})', data_df_one.iloc[target_idx, 1].replace(',', ''))\n",
    "        overseas_fee_match = re.search(r'£(\\d{5})', data_df_one.iloc[target_idx, 2].replace(',', ''))\n",
    "        home_fee = int(home_fee_match.group(1)) if home_fee_match else None\n",
    "        overseas_fee = int(overseas_fee_match.group(1)) if overseas_fee_match else None\n",
    "\n",
    "# Add the extracted row to the top of the processed_df DataFrame\n",
    "additional_row = pd.DataFrame({\n",
    "    'Department': [f\"All {year}\"],\n",
    "    'Program': [\"UG Degree\"],\n",
    "    'Home fees': [home_fee],\n",
    "    'Overseas fees': [overseas_fee]\n",
    "})\n",
    "\n",
    "# Append the additional row to the processed_df DataFrame\n",
    "processed_df = pd.concat([additional_row, processed_df]).reset_index(drop=True)\n",
    "\n",
    "# Display the final DataFrame including the new row\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f023a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294269a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b447c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2236c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/1359154016.py:31: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
      "/var/folders/w_/ccm86j116md1g5bmpdbhyv5r0000gn/T/ipykernel_69042/1359154016.py:31: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
      "Skipping line 7: expected 3 fields, saw 5\n",
      "Skipping line 8: expected 3 fields, saw 5\n",
      "Skipping line 9: expected 3 fields, saw 5\n",
      "Skipping line 10: expected 3 fields, saw 5\n",
      "Skipping line 11: expected 3 fields, saw 5\n",
      "Skipping line 12: expected 3 fields, saw 5\n",
      "Skipping line 13: expected 3 fields, saw 5\n",
      "Skipping line 14: expected 3 fields, saw 5\n",
      "Skipping line 15: expected 3 fields, saw 5\n",
      "Skipping line 16: expected 3 fields, saw 5\n",
      "Skipping line 17: expected 3 fields, saw 5\n",
      "Skipping line 18: expected 3 fields, saw 5\n",
      "Skipping line 19: expected 3 fields, saw 5\n",
      "Skipping line 20: expected 3 fields, saw 5\n",
      "Skipping line 21: expected 3 fields, saw 5\n",
      "Skipping line 22: expected 3 fields, saw 5\n",
      "Skipping line 23: expected 3 fields, saw 5\n",
      "Skipping line 24: expected 3 fields, saw 5\n",
      "Skipping line 25: expected 3 fields, saw 5\n",
      "Skipping line 26: expected 3 fields, saw 5\n",
      "Skipping line 27: expected 3 fields, saw 5\n",
      "Skipping line 28: expected 3 fields, saw 5\n",
      "Skipping line 29: expected 3 fields, saw 5\n",
      "Skipping line 30: expected 3 fields, saw 5\n",
      "Skipping line 31: expected 3 fields, saw 5\n",
      "Skipping line 32: expected 3 fields, saw 5\n",
      "Skipping line 33: expected 3 fields, saw 5\n",
      "Skipping line 34: expected 3 fields, saw 5\n",
      "Skipping line 35: expected 3 fields, saw 5\n",
      "Skipping line 36: expected 3 fields, saw 5\n",
      "Skipping line 37: expected 3 fields, saw 5\n",
      "Skipping line 38: expected 3 fields, saw 5\n",
      "Skipping line 39: expected 3 fields, saw 5\n",
      "Skipping line 40: expected 3 fields, saw 5\n",
      "Skipping line 41: expected 3 fields, saw 5\n",
      "Skipping line 42: expected 3 fields, saw 5\n",
      "Skipping line 43: expected 3 fields, saw 5\n",
      "Skipping line 44: expected 3 fields, saw 5\n",
      "Skipping line 45: expected 3 fields, saw 5\n",
      "Skipping line 46: expected 3 fields, saw 5\n",
      "Skipping line 47: expected 3 fields, saw 5\n",
      "Skipping line 48: expected 3 fields, saw 5\n",
      "Skipping line 49: expected 3 fields, saw 5\n",
      "Skipping line 50: expected 3 fields, saw 5\n",
      "Skipping line 51: expected 3 fields, saw 5\n",
      "Skipping line 52: expected 3 fields, saw 5\n",
      "Skipping line 53: expected 3 fields, saw 5\n",
      "Skipping line 54: expected 3 fields, saw 5\n",
      "Skipping line 55: expected 3 fields, saw 5\n",
      "Skipping line 56: expected 3 fields, saw 5\n",
      "Skipping line 57: expected 3 fields, saw 5\n",
      "Skipping line 58: expected 3 fields, saw 5\n",
      "Skipping line 59: expected 3 fields, saw 5\n",
      "Skipping line 60: expected 3 fields, saw 5\n",
      "Skipping line 61: expected 3 fields, saw 5\n",
      "Skipping line 62: expected 3 fields, saw 5\n",
      "Skipping line 63: expected 3 fields, saw 5\n",
      "Skipping line 65: expected 3 fields, saw 5\n",
      "Skipping line 66: expected 3 fields, saw 5\n",
      "Skipping line 67: expected 3 fields, saw 5\n",
      "Skipping line 68: expected 3 fields, saw 5\n",
      "Skipping line 69: expected 3 fields, saw 5\n",
      "Skipping line 70: expected 3 fields, saw 5\n",
      "Skipping line 71: expected 3 fields, saw 5\n",
      "Skipping line 72: expected 3 fields, saw 5\n",
      "Skipping line 73: expected 3 fields, saw 5\n",
      "Skipping line 74: expected 3 fields, saw 5\n",
      "Skipping line 75: expected 3 fields, saw 5\n",
      "Skipping line 76: expected 3 fields, saw 5\n",
      "Skipping line 77: expected 3 fields, saw 5\n",
      "Skipping line 78: expected 3 fields, saw 5\n",
      "Skipping line 79: expected 3 fields, saw 5\n",
      "Skipping line 80: expected 3 fields, saw 5\n",
      "Skipping line 81: expected 3 fields, saw 5\n",
      "Skipping line 82: expected 3 fields, saw 5\n",
      "Skipping line 83: expected 3 fields, saw 5\n",
      "Skipping line 84: expected 3 fields, saw 5\n",
      "Skipping line 85: expected 3 fields, saw 5\n",
      "Skipping line 86: expected 3 fields, saw 5\n",
      "Skipping line 87: expected 3 fields, saw 5\n",
      "Skipping line 88: expected 3 fields, saw 5\n",
      "Skipping line 89: expected 3 fields, saw 5\n",
      "Skipping line 90: expected 3 fields, saw 5\n",
      "Skipping line 91: expected 3 fields, saw 5\n",
      "Skipping line 92: expected 3 fields, saw 5\n",
      "Skipping line 93: expected 3 fields, saw 5\n",
      "Skipping line 94: expected 3 fields, saw 5\n",
      "Skipping line 95: expected 3 fields, saw 5\n",
      "Skipping line 96: expected 3 fields, saw 5\n",
      "Skipping line 97: expected 3 fields, saw 5\n",
      "Skipping line 98: expected 3 fields, saw 5\n",
      "Skipping line 99: expected 3 fields, saw 5\n",
      "Skipping line 100: expected 3 fields, saw 5\n",
      "Skipping line 101: expected 3 fields, saw 5\n",
      "Skipping line 102: expected 3 fields, saw 5\n",
      "Skipping line 103: expected 3 fields, saw 5\n",
      "Skipping line 104: expected 3 fields, saw 5\n",
      "Skipping line 105: expected 3 fields, saw 5\n",
      "Skipping line 106: expected 3 fields, saw 5\n",
      "Skipping line 107: expected 3 fields, saw 5\n",
      "Skipping line 108: expected 3 fields, saw 5\n",
      "Skipping line 109: expected 3 fields, saw 5\n",
      "Skipping line 110: expected 3 fields, saw 5\n",
      "Skipping line 111: expected 3 fields, saw 5\n",
      "Skipping line 112: expected 3 fields, saw 5\n",
      "Skipping line 113: expected 3 fields, saw 5\n",
      "Skipping line 114: expected 3 fields, saw 5\n",
      "Skipping line 115: expected 3 fields, saw 5\n",
      "Skipping line 116: expected 3 fields, saw 5\n",
      "Skipping line 117: expected 3 fields, saw 5\n",
      "Skipping line 118: expected 3 fields, saw 5\n",
      "Skipping line 119: expected 3 fields, saw 5\n",
      "Skipping line 120: expected 3 fields, saw 5\n",
      "Skipping line 121: expected 3 fields, saw 5\n",
      "Skipping line 122: expected 3 fields, saw 5\n",
      "Skipping line 123: expected 3 fields, saw 5\n",
      "Skipping line 124: expected 3 fields, saw 5\n",
      "Skipping line 125: expected 3 fields, saw 5\n",
      "Skipping line 126: expected 3 fields, saw 5\n",
      "Skipping line 127: expected 3 fields, saw 5\n",
      "Skipping line 128: expected 3 fields, saw 5\n",
      "Skipping line 129: expected 3 fields, saw 5\n",
      "Skipping line 130: expected 3 fields, saw 5\n",
      "Skipping line 131: expected 3 fields, saw 5\n",
      "Skipping line 132: expected 3 fields, saw 5\n",
      "Skipping line 133: expected 3 fields, saw 5\n",
      "Skipping line 134: expected 3 fields, saw 5\n",
      "Skipping line 135: expected 3 fields, saw 5\n",
      "Skipping line 136: expected 3 fields, saw 5\n",
      "Skipping line 137: expected 3 fields, saw 5\n",
      "Skipping line 138: expected 3 fields, saw 5\n",
      "Skipping line 139: expected 3 fields, saw 5\n",
      "Skipping line 140: expected 3 fields, saw 5\n",
      "Skipping line 141: expected 3 fields, saw 5\n",
      "Skipping line 142: expected 3 fields, saw 5\n",
      "Skipping line 143: expected 3 fields, saw 5\n",
      "Skipping line 144: expected 3 fields, saw 5\n",
      "Skipping line 145: expected 3 fields, saw 5\n",
      "Skipping line 146: expected 3 fields, saw 5\n",
      "Skipping line 147: expected 3 fields, saw 5\n",
      "Skipping line 148: expected 3 fields, saw 5\n",
      "Skipping line 149: expected 3 fields, saw 5\n",
      "Skipping line 150: expected 3 fields, saw 5\n",
      "Skipping line 151: expected 3 fields, saw 5\n",
      "Skipping line 152: expected 3 fields, saw 5\n",
      "Skipping line 153: expected 3 fields, saw 5\n",
      "Skipping line 154: expected 3 fields, saw 5\n",
      "Skipping line 155: expected 3 fields, saw 5\n",
      "Skipping line 156: expected 3 fields, saw 5\n",
      "Skipping line 157: expected 3 fields, saw 5\n",
      "Skipping line 158: expected 3 fields, saw 5\n",
      "Skipping line 159: expected 3 fields, saw 5\n",
      "Skipping line 160: expected 3 fields, saw 5\n",
      "Skipping line 161: expected 3 fields, saw 5\n",
      "Skipping line 162: expected 3 fields, saw 5\n",
      "Skipping line 163: expected 3 fields, saw 5\n",
      "Skipping line 164: expected 3 fields, saw 5\n",
      "Skipping line 165: expected 3 fields, saw 5\n",
      "Skipping line 166: expected 3 fields, saw 5\n",
      "Skipping line 167: expected 3 fields, saw 5\n",
      "Skipping line 168: expected 3 fields, saw 5\n",
      "Skipping line 169: expected 3 fields, saw 5\n",
      "Skipping line 170: expected 3 fields, saw 5\n",
      "Skipping line 171: expected 3 fields, saw 5\n",
      "Skipping line 172: expected 3 fields, saw 5\n",
      "Skipping line 173: expected 3 fields, saw 5\n",
      "Skipping line 174: expected 3 fields, saw 5\n",
      "Skipping line 175: expected 3 fields, saw 5\n",
      "Skipping line 176: expected 3 fields, saw 5\n",
      "Skipping line 177: expected 3 fields, saw 5\n",
      "Skipping line 178: expected 3 fields, saw 5\n",
      "Skipping line 179: expected 3 fields, saw 5\n",
      "Skipping line 180: expected 3 fields, saw 5\n",
      "Skipping line 181: expected 3 fields, saw 5\n",
      "Skipping line 182: expected 3 fields, saw 5\n",
      "Skipping line 183: expected 3 fields, saw 5\n",
      "Skipping line 184: expected 3 fields, saw 5\n",
      "Skipping line 185: expected 3 fields, saw 5\n",
      "Skipping line 186: expected 3 fields, saw 5\n",
      "Skipping line 187: expected 3 fields, saw 5\n",
      "Skipping line 188: expected 3 fields, saw 5\n",
      "Skipping line 189: expected 3 fields, saw 5\n",
      "Skipping line 190: expected 3 fields, saw 5\n",
      "Skipping line 191: expected 3 fields, saw 5\n",
      "Skipping line 192: expected 3 fields, saw 5\n",
      "Skipping line 193: expected 3 fields, saw 5\n",
      "Skipping line 194: expected 3 fields, saw 5\n",
      "Skipping line 195: expected 3 fields, saw 5\n",
      "Skipping line 196: expected 3 fields, saw 5\n",
      "Skipping line 197: expected 3 fields, saw 5\n",
      "Skipping line 198: expected 3 fields, saw 5\n",
      "Skipping line 199: expected 3 fields, saw 5\n",
      "Skipping line 200: expected 3 fields, saw 5\n",
      "Skipping line 201: expected 3 fields, saw 5\n",
      "Skipping line 202: expected 3 fields, saw 5\n",
      "Skipping line 203: expected 3 fields, saw 5\n",
      "Skipping line 204: expected 3 fields, saw 5\n",
      "Skipping line 205: expected 3 fields, saw 5\n",
      "Skipping line 206: expected 3 fields, saw 5\n",
      "Skipping line 207: expected 3 fields, saw 5\n",
      "Skipping line 208: expected 3 fields, saw 5\n",
      "Skipping line 209: expected 3 fields, saw 5\n",
      "Skipping line 210: expected 3 fields, saw 5\n",
      "Skipping line 211: expected 3 fields, saw 5\n",
      "Skipping line 212: expected 3 fields, saw 5\n",
      "Skipping line 213: expected 3 fields, saw 5\n",
      "Skipping line 214: expected 3 fields, saw 5\n",
      "Skipping line 215: expected 3 fields, saw 5\n",
      "Skipping line 216: expected 3 fields, saw 5\n",
      "Skipping line 217: expected 3 fields, saw 5\n",
      "Skipping line 218: expected 3 fields, saw 5\n",
      "Skipping line 219: expected 3 fields, saw 5\n",
      "Skipping line 220: expected 3 fields, saw 5\n",
      "Skipping line 221: expected 3 fields, saw 5\n",
      "Skipping line 222: expected 3 fields, saw 5\n",
      "Skipping line 223: expected 3 fields, saw 5\n",
      "Skipping line 224: expected 3 fields, saw 5\n",
      "Skipping line 225: expected 3 fields, saw 5\n",
      "Skipping line 226: expected 3 fields, saw 5\n",
      "Skipping line 227: expected 3 fields, saw 5\n",
      "Skipping line 228: expected 3 fields, saw 5\n",
      "Skipping line 229: expected 3 fields, saw 5\n",
      "Skipping line 230: expected 3 fields, saw 5\n",
      "Skipping line 231: expected 3 fields, saw 5\n",
      "Skipping line 232: expected 3 fields, saw 5\n",
      "Skipping line 233: expected 3 fields, saw 5\n",
      "Skipping line 234: expected 3 fields, saw 5\n",
      "Skipping line 235: expected 3 fields, saw 5\n",
      "Skipping line 236: expected 3 fields, saw 5\n",
      "Skipping line 237: expected 3 fields, saw 5\n",
      "Skipping line 238: expected 3 fields, saw 5\n",
      "Skipping line 239: expected 3 fields, saw 5\n",
      "Skipping line 240: expected 3 fields, saw 5\n",
      "Skipping line 241: expected 3 fields, saw 5\n",
      "Skipping line 242: expected 3 fields, saw 5\n",
      "Skipping line 243: expected 3 fields, saw 5\n",
      "Skipping line 244: expected 3 fields, saw 5\n",
      "Skipping line 245: expected 3 fields, saw 5\n",
      "Skipping line 246: expected 3 fields, saw 5\n",
      "Skipping line 247: expected 3 fields, saw 5\n",
      "Skipping line 248: expected 3 fields, saw 5\n",
      "Skipping line 249: expected 3 fields, saw 5\n",
      "Skipping line 250: expected 3 fields, saw 5\n",
      "Skipping line 251: expected 3 fields, saw 5\n",
      "Skipping line 252: expected 3 fields, saw 5\n",
      "Skipping line 253: expected 3 fields, saw 5\n",
      "Skipping line 254: expected 3 fields, saw 5\n",
      "Skipping line 255: expected 3 fields, saw 5\n",
      "Skipping line 256: expected 3 fields, saw 5\n",
      "Skipping line 257: expected 3 fields, saw 5\n",
      "Skipping line 258: expected 3 fields, saw 5\n",
      "Skipping line 259: expected 3 fields, saw 5\n",
      "Skipping line 260: expected 3 fields, saw 5\n",
      "Skipping line 261: expected 3 fields, saw 5\n",
      "Skipping line 262: expected 3 fields, saw 5\n",
      "Skipping line 263: expected 3 fields, saw 5\n",
      "Skipping line 264: expected 3 fields, saw 5\n",
      "Skipping line 265: expected 3 fields, saw 5\n",
      "Skipping line 266: expected 3 fields, saw 5\n",
      "Skipping line 267: expected 3 fields, saw 5\n",
      "Skipping line 268: expected 3 fields, saw 5\n",
      "Skipping line 269: expected 3 fields, saw 5\n",
      "Skipping line 270: expected 3 fields, saw 5\n",
      "Skipping line 271: expected 3 fields, saw 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Department</th>\n",
       "      <th>Program</th>\n",
       "      <th>Home fees</th>\n",
       "      <th>Overseas fees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All 2019</td>\n",
       "      <td>UG Degree</td>\n",
       "      <td>9250</td>\n",
       "      <td>19920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Department    Program  Home fees  Overseas fees\n",
       "0   All 2019  UG Degree       9250          19920"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2021\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, output_csv_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        all_tables = []\n",
    "        # Iterate through each page of the PDF\n",
    "        for page in pdf.pages:\n",
    "            # Extract tables from the current page\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                all_tables.extend(table)  # Add the rows of the table to all_tables list\n",
    "       \n",
    "        # Write all extracted tables to a CSV file\n",
    "        with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for row in all_tables:\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Specify the path to your PDF and the output CSV file\n",
    "pdf_path = 'Data/ToF-3Aug21FinalComb.pdf'\n",
    "output_csv_path = 'Data/2021_Fees.csv'\n",
    "\n",
    "extract_tables_from_pdf(pdf_path, output_csv_path)\n",
    "\n",
    "\n",
    "# Load the CSV file containing the tuition fees and program data\n",
    "output_csv_path = 'Data/2021_Fees.csv'  \n",
    "csv_path = 'Data/Florian_Wirtz_eigentlich_noch_was_mit_der_v2.csv'  \n",
    "\n",
    "# Read the CSV files\n",
    "data_df_one = pd.read_csv(output_csv_path, error_bad_lines=False, warn_bad_lines=True)\n",
    "data_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Combine 'Department' and 'Program' into a new column for unique combinations\n",
    "data_df['Dept_Program'] = data_df['Department'] + \" \" + data_df['Program']\n",
    "\n",
    "# Get unique combinations\n",
    "unique_dept_programs = data_df['Dept_Program'].unique()\n",
    "\n",
    "# Dictionary to store original to transformed mappings\n",
    "original_to_transformed = {}\n",
    "\n",
    "# Process only Master's programs and adjust department names\n",
    "processed_dept_programs = []\n",
    "for combo in unique_dept_programs:\n",
    "    if isinstance(combo, str):  # Check if the item is a string\n",
    "        if \"PG Taught\" in combo:\n",
    "            original_department = combo.replace(\" PG Taught\", \"\")\n",
    "            transformed_department = original_department\n",
    "            # Handle specific naming transformations\n",
    "            if \"And\" in transformed_department:\n",
    "                transformed_department = transformed_department.split(\"And\")[0].strip()\n",
    "            if transformed_department == \"International History\":\n",
    "                transformed_department = \"History\"\n",
    "            if transformed_department == \"European Institute\":\n",
    "                transformed_department = \"European\"\n",
    "            if transformed_department == \"Law School\" or transformed_department == \"Law\":\n",
    "                transformed_department = \"LLM\"\n",
    "            if transformed_department == \"Philosophy Logic\":\n",
    "                transformed_department = \"Philosophy\"\n",
    "            if transformed_department == \"School of Public Policy\":\n",
    "                transformed_department = \"Public Policy\"\n",
    "            if transformed_department == \"Gender Studies\":\n",
    "                transformed_department = \"Gender\"\n",
    "            if \"Psychological\" in transformed_department:\n",
    "                transformed_department = \"Psychology\"\n",
    "            \n",
    "            # Save mapping\n",
    "            original_to_transformed[transformed_department] = original_department\n",
    "            processed_dept_programs.append(transformed_department)\n",
    "\n",
    "# Create DataFrame from processed list\n",
    "processed_df = pd.DataFrame(processed_dept_programs, columns=['Department'])\n",
    "\n",
    "# Insert \"Program\" column with \"PG Taught\" as the value for all entries\n",
    "processed_df.insert(1, 'Program', 'PG Taught')\n",
    "\n",
    "def find_matching_data(dept_program):\n",
    "    matches = data_df_one[data_df_one.iloc[:, 0].str.contains(dept_program, na=False)]\n",
    "    if not matches.empty:\n",
    "        col2_values = []\n",
    "        col3_values = []\n",
    "        for _, row in matches.iterrows():\n",
    "            # Process and clean fee data for averaging\n",
    "            try:\n",
    "                cleaned_value_col2 = float(str(row[1]).replace('£', '').replace(',', ''))\n",
    "                if cleaned_value_col2.is_integer():\n",
    "                    col2_values.append(int(cleaned_value_col2))\n",
    "            except ValueError:\n",
    "                col2_values.append(pd.NA)\n",
    "            try:\n",
    "                cleaned_value_col3 = float(str(row[2]).replace('£', '').replace(',', ''))\n",
    "                if cleaned_value_col3.is_integer():\n",
    "                    col3_values.append(int(cleaned_value_col3))\n",
    "            except ValueError:\n",
    "                col3_values.append(pd.NA)\n",
    "        \n",
    "        # Calculate averages while ignoring N/A values\n",
    "        avg_col2 = pd.Series(col2_values).dropna().mean()\n",
    "        avg_col3 = pd.Series(col3_values).dropna().mean()\n",
    "        return pd.Series([avg_col2, avg_col3])\n",
    "    return pd.Series([pd.NA, pd.NA])\n",
    "\n",
    "# Apply the function to find and average matching tuition fees\n",
    "processed_df[['Home fees', 'Overseas fees']] = processed_df['Department'].apply(find_matching_data)\n",
    "\n",
    "# Remove rows where either column contains NA or NaN values\n",
    "processed_df.dropna(subset=['Home fees', 'Overseas fees'], inplace=True)\n",
    "\n",
    "# Ensure all remaining values are integers\n",
    "processed_df['Home fees'] = processed_df['Home fees'].astype(int)\n",
    "processed_df['Overseas fees'] = processed_df['Overseas fees'].astype(int)\n",
    "\n",
    "# Revert department names to original values\n",
    "processed_df['Department'] = processed_df['Department'].map(original_to_transformed)\n",
    "\n",
    "# Display the final DataFrame\n",
    "processed_df\n",
    "\n",
    "# Find the row index for the specific phrase and extract the fees and year\n",
    "idx = data_df_one.index[data_df_one.iloc[:, 0].str.contains(\"Students commencing their degree in\" or \"New Entrants\", na=False)].tolist()\n",
    "if idx:\n",
    "    target_idx = idx[0]  # Assume the first occurrence\n",
    "    if \"Undergraduate\" in data_df_one.iloc[target_idx - 1, 0]:\n",
    "        year_match = re.search(r'\\b(\\d{4})\\b', data_df_one.iloc[target_idx, 0])\n",
    "        year = year_match.group(0) if year_match else \"Unknown\"\n",
    "        home_fee_match = re.search(r'£(\\d{4})', data_df_one.iloc[target_idx, 1].replace(',', ''))\n",
    "        overseas_fee_match = re.search(r'£(\\d{5})', data_df_one.iloc[target_idx, 2].replace(',', ''))\n",
    "        home_fee = int(home_fee_match.group(1)) if home_fee_match else None\n",
    "        overseas_fee = int(overseas_fee_match.group(1)) if overseas_fee_match else None\n",
    "\n",
    "# Add the extracted row to the top of the processed_df DataFrame\n",
    "additional_row = pd.DataFrame({\n",
    "    'Department': [f\"All {year}\"],\n",
    "    'Program': [\"UG Degree\"],\n",
    "    'Home fees': [home_fee],\n",
    "    'Overseas fees': [overseas_fee]\n",
    "})\n",
    "\n",
    "# Append the additional row to the processed_df DataFrame\n",
    "processed_df = pd.concat([additional_row, processed_df]).reset_index(drop=True)\n",
    "\n",
    "# Display the final DataFrame including the new row\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13cb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538e2e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff23bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped a malformed table\n",
      "Skipped a malformed table\n",
      "                                             Column1    Column2  \\\n",
      "0                        1\\nUndergraduate Programmes        NaN   \n",
      "1                                                NaN  Full time   \n",
      "2                                                NaN       Home   \n",
      "3  Undergraduate first degree - New entrants in 2...        NaN   \n",
      "4  Home students (does not include EU students) c...   £9,250 2   \n",
      "\n",
      "                    Column3  \n",
      "0                       NaN  \n",
      "1                       NaN  \n",
      "2  Overseas\\n(including EU)  \n",
      "3                       NaN  \n",
      "4                       NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 7: expected 3 fields, saw 5\n",
      "Skipping line 8: expected 3 fields, saw 5\n",
      "Skipping line 9: expected 3 fields, saw 5\n",
      "Skipping line 10: expected 3 fields, saw 5\n",
      "Skipping line 11: expected 3 fields, saw 5\n",
      "Skipping line 12: expected 3 fields, saw 5\n",
      "Skipping line 13: expected 3 fields, saw 5\n",
      "Skipping line 14: expected 3 fields, saw 5\n",
      "Skipping line 15: expected 3 fields, saw 5\n",
      "Skipping line 16: expected 3 fields, saw 5\n",
      "Skipping line 17: expected 3 fields, saw 5\n",
      "Skipping line 18: expected 3 fields, saw 5\n",
      "Skipping line 19: expected 3 fields, saw 5\n",
      "Skipping line 20: expected 3 fields, saw 5\n",
      "Skipping line 21: expected 3 fields, saw 5\n",
      "Skipping line 22: expected 3 fields, saw 5\n",
      "Skipping line 23: expected 3 fields, saw 5\n",
      "Skipping line 24: expected 3 fields, saw 5\n",
      "Skipping line 25: expected 3 fields, saw 5\n",
      "Skipping line 26: expected 3 fields, saw 5\n",
      "Skipping line 27: expected 3 fields, saw 5\n",
      "Skipping line 28: expected 3 fields, saw 5\n",
      "Skipping line 29: expected 3 fields, saw 5\n",
      "Skipping line 30: expected 3 fields, saw 5\n",
      "Skipping line 31: expected 3 fields, saw 5\n",
      "Skipping line 32: expected 3 fields, saw 5\n",
      "Skipping line 33: expected 3 fields, saw 5\n",
      "Skipping line 34: expected 3 fields, saw 5\n",
      "Skipping line 35: expected 3 fields, saw 5\n",
      "Skipping line 36: expected 3 fields, saw 5\n",
      "Skipping line 37: expected 3 fields, saw 5\n",
      "Skipping line 38: expected 3 fields, saw 5\n",
      "Skipping line 39: expected 3 fields, saw 5\n",
      "Skipping line 40: expected 3 fields, saw 5\n",
      "Skipping line 41: expected 3 fields, saw 5\n",
      "Skipping line 42: expected 3 fields, saw 5\n",
      "Skipping line 43: expected 3 fields, saw 5\n",
      "Skipping line 44: expected 3 fields, saw 5\n",
      "Skipping line 45: expected 3 fields, saw 5\n",
      "Skipping line 46: expected 3 fields, saw 5\n",
      "Skipping line 47: expected 3 fields, saw 5\n",
      "Skipping line 48: expected 3 fields, saw 5\n",
      "Skipping line 49: expected 3 fields, saw 5\n",
      "Skipping line 50: expected 3 fields, saw 5\n",
      "Skipping line 51: expected 3 fields, saw 5\n",
      "Skipping line 52: expected 3 fields, saw 5\n",
      "Skipping line 53: expected 3 fields, saw 5\n",
      "Skipping line 54: expected 3 fields, saw 5\n",
      "Skipping line 55: expected 3 fields, saw 5\n",
      "Skipping line 56: expected 3 fields, saw 5\n",
      "Skipping line 57: expected 3 fields, saw 5\n",
      "Skipping line 58: expected 3 fields, saw 5\n",
      "Skipping line 59: expected 3 fields, saw 5\n",
      "Skipping line 60: expected 3 fields, saw 5\n",
      "Skipping line 61: expected 3 fields, saw 5\n",
      "Skipping line 62: expected 3 fields, saw 5\n",
      "Skipping line 63: expected 3 fields, saw 5\n",
      "Skipping line 64: expected 3 fields, saw 5\n",
      "Skipping line 65: expected 3 fields, saw 5\n",
      "Skipping line 66: expected 3 fields, saw 5\n",
      "Skipping line 67: expected 3 fields, saw 5\n",
      "Skipping line 68: expected 3 fields, saw 5\n",
      "Skipping line 69: expected 3 fields, saw 5\n",
      "Skipping line 70: expected 3 fields, saw 5\n",
      "Skipping line 71: expected 3 fields, saw 5\n",
      "Skipping line 72: expected 3 fields, saw 5\n",
      "Skipping line 73: expected 3 fields, saw 5\n",
      "Skipping line 74: expected 3 fields, saw 5\n",
      "Skipping line 75: expected 3 fields, saw 5\n",
      "Skipping line 76: expected 3 fields, saw 5\n",
      "Skipping line 77: expected 3 fields, saw 5\n",
      "Skipping line 78: expected 3 fields, saw 5\n",
      "Skipping line 79: expected 3 fields, saw 5\n",
      "Skipping line 80: expected 3 fields, saw 5\n",
      "Skipping line 81: expected 3 fields, saw 5\n",
      "Skipping line 82: expected 3 fields, saw 5\n",
      "Skipping line 83: expected 3 fields, saw 5\n",
      "Skipping line 84: expected 3 fields, saw 5\n",
      "Skipping line 85: expected 3 fields, saw 5\n",
      "Skipping line 86: expected 3 fields, saw 5\n",
      "Skipping line 87: expected 3 fields, saw 5\n",
      "Skipping line 88: expected 3 fields, saw 5\n",
      "Skipping line 89: expected 3 fields, saw 5\n",
      "Skipping line 90: expected 3 fields, saw 5\n",
      "Skipping line 91: expected 3 fields, saw 5\n",
      "Skipping line 92: expected 3 fields, saw 5\n",
      "Skipping line 93: expected 3 fields, saw 5\n",
      "Skipping line 94: expected 3 fields, saw 5\n",
      "Skipping line 95: expected 3 fields, saw 5\n",
      "Skipping line 96: expected 3 fields, saw 5\n",
      "Skipping line 97: expected 3 fields, saw 5\n",
      "Skipping line 98: expected 3 fields, saw 5\n",
      "Skipping line 99: expected 3 fields, saw 5\n",
      "Skipping line 100: expected 3 fields, saw 5\n",
      "Skipping line 101: expected 3 fields, saw 5\n",
      "Skipping line 102: expected 3 fields, saw 5\n",
      "Skipping line 103: expected 3 fields, saw 5\n",
      "Skipping line 104: expected 3 fields, saw 5\n",
      "Skipping line 105: expected 3 fields, saw 5\n",
      "Skipping line 106: expected 3 fields, saw 5\n",
      "Skipping line 107: expected 3 fields, saw 5\n",
      "Skipping line 108: expected 3 fields, saw 5\n",
      "Skipping line 109: expected 3 fields, saw 5\n",
      "Skipping line 110: expected 3 fields, saw 5\n",
      "Skipping line 111: expected 3 fields, saw 5\n",
      "Skipping line 112: expected 3 fields, saw 5\n",
      "Skipping line 113: expected 3 fields, saw 5\n",
      "Skipping line 114: expected 3 fields, saw 5\n",
      "Skipping line 115: expected 3 fields, saw 5\n",
      "Skipping line 116: expected 3 fields, saw 5\n",
      "Skipping line 117: expected 3 fields, saw 5\n",
      "Skipping line 118: expected 3 fields, saw 5\n",
      "Skipping line 119: expected 3 fields, saw 5\n",
      "Skipping line 120: expected 3 fields, saw 5\n",
      "Skipping line 121: expected 3 fields, saw 5\n",
      "Skipping line 122: expected 3 fields, saw 5\n",
      "Skipping line 123: expected 3 fields, saw 5\n",
      "Skipping line 124: expected 3 fields, saw 5\n",
      "Skipping line 125: expected 3 fields, saw 5\n",
      "Skipping line 126: expected 3 fields, saw 5\n",
      "Skipping line 127: expected 3 fields, saw 5\n",
      "Skipping line 128: expected 3 fields, saw 5\n",
      "Skipping line 129: expected 3 fields, saw 5\n",
      "Skipping line 130: expected 3 fields, saw 5\n",
      "Skipping line 131: expected 3 fields, saw 5\n",
      "Skipping line 132: expected 3 fields, saw 5\n",
      "Skipping line 133: expected 3 fields, saw 5\n",
      "Skipping line 134: expected 3 fields, saw 5\n",
      "Skipping line 135: expected 3 fields, saw 5\n",
      "Skipping line 136: expected 3 fields, saw 5\n",
      "Skipping line 137: expected 3 fields, saw 5\n",
      "Skipping line 138: expected 3 fields, saw 5\n",
      "Skipping line 139: expected 3 fields, saw 5\n",
      "Skipping line 140: expected 3 fields, saw 5\n",
      "Skipping line 141: expected 3 fields, saw 5\n",
      "Skipping line 142: expected 3 fields, saw 5\n",
      "Skipping line 143: expected 3 fields, saw 5\n",
      "Skipping line 144: expected 3 fields, saw 5\n",
      "Skipping line 145: expected 3 fields, saw 5\n",
      "Skipping line 146: expected 3 fields, saw 5\n",
      "Skipping line 147: expected 3 fields, saw 5\n",
      "Skipping line 148: expected 3 fields, saw 5\n",
      "Skipping line 149: expected 3 fields, saw 5\n",
      "Skipping line 150: expected 3 fields, saw 5\n",
      "Skipping line 151: expected 3 fields, saw 5\n",
      "Skipping line 152: expected 3 fields, saw 5\n",
      "Skipping line 153: expected 3 fields, saw 5\n",
      "Skipping line 154: expected 3 fields, saw 5\n",
      "Skipping line 155: expected 3 fields, saw 5\n",
      "Skipping line 156: expected 3 fields, saw 5\n",
      "Skipping line 157: expected 3 fields, saw 5\n",
      "Skipping line 158: expected 3 fields, saw 5\n",
      "Skipping line 159: expected 3 fields, saw 5\n",
      "Skipping line 160: expected 3 fields, saw 5\n",
      "Skipping line 161: expected 3 fields, saw 5\n",
      "Skipping line 162: expected 3 fields, saw 5\n",
      "Skipping line 163: expected 3 fields, saw 5\n",
      "Skipping line 164: expected 3 fields, saw 5\n",
      "Skipping line 165: expected 3 fields, saw 5\n",
      "Skipping line 166: expected 3 fields, saw 5\n",
      "Skipping line 167: expected 3 fields, saw 5\n",
      "Skipping line 168: expected 3 fields, saw 5\n",
      "Skipping line 169: expected 3 fields, saw 5\n",
      "Skipping line 170: expected 3 fields, saw 5\n",
      "Skipping line 171: expected 3 fields, saw 5\n",
      "Skipping line 172: expected 3 fields, saw 5\n",
      "Skipping line 173: expected 3 fields, saw 5\n",
      "Skipping line 174: expected 3 fields, saw 5\n",
      "Skipping line 175: expected 3 fields, saw 5\n",
      "Skipping line 176: expected 3 fields, saw 5\n",
      "Skipping line 177: expected 3 fields, saw 5\n",
      "Skipping line 178: expected 3 fields, saw 5\n",
      "Skipping line 179: expected 3 fields, saw 5\n",
      "Skipping line 180: expected 3 fields, saw 5\n",
      "Skipping line 181: expected 3 fields, saw 5\n",
      "Skipping line 182: expected 3 fields, saw 5\n",
      "Skipping line 183: expected 3 fields, saw 5\n",
      "Skipping line 184: expected 3 fields, saw 5\n",
      "Skipping line 185: expected 3 fields, saw 5\n",
      "Skipping line 186: expected 3 fields, saw 5\n",
      "Skipping line 187: expected 3 fields, saw 5\n",
      "Skipping line 188: expected 3 fields, saw 5\n",
      "Skipping line 189: expected 3 fields, saw 5\n",
      "Skipping line 190: expected 3 fields, saw 5\n",
      "Skipping line 191: expected 3 fields, saw 5\n",
      "Skipping line 192: expected 3 fields, saw 5\n",
      "Skipping line 193: expected 3 fields, saw 5\n",
      "Skipping line 194: expected 3 fields, saw 5\n",
      "Skipping line 195: expected 3 fields, saw 5\n",
      "Skipping line 196: expected 3 fields, saw 5\n",
      "Skipping line 197: expected 3 fields, saw 5\n",
      "Skipping line 198: expected 3 fields, saw 5\n",
      "Skipping line 199: expected 3 fields, saw 5\n",
      "Skipping line 200: expected 3 fields, saw 5\n",
      "Skipping line 201: expected 3 fields, saw 5\n",
      "Skipping line 202: expected 3 fields, saw 5\n",
      "Skipping line 203: expected 3 fields, saw 5\n",
      "Skipping line 204: expected 3 fields, saw 5\n",
      "Skipping line 205: expected 3 fields, saw 5\n",
      "Skipping line 206: expected 3 fields, saw 5\n",
      "Skipping line 207: expected 3 fields, saw 5\n",
      "Skipping line 208: expected 3 fields, saw 5\n",
      "Skipping line 209: expected 3 fields, saw 5\n",
      "Skipping line 210: expected 3 fields, saw 5\n",
      "Skipping line 211: expected 3 fields, saw 5\n",
      "Skipping line 212: expected 3 fields, saw 5\n",
      "Skipping line 213: expected 3 fields, saw 5\n",
      "Skipping line 214: expected 3 fields, saw 5\n",
      "Skipping line 215: expected 3 fields, saw 5\n",
      "Skipping line 216: expected 3 fields, saw 5\n",
      "Skipping line 217: expected 3 fields, saw 5\n",
      "Skipping line 218: expected 3 fields, saw 5\n",
      "Skipping line 219: expected 3 fields, saw 5\n",
      "Skipping line 220: expected 3 fields, saw 5\n",
      "Skipping line 221: expected 3 fields, saw 5\n",
      "Skipping line 222: expected 3 fields, saw 5\n",
      "Skipping line 223: expected 3 fields, saw 5\n",
      "Skipping line 224: expected 3 fields, saw 5\n",
      "Skipping line 225: expected 3 fields, saw 5\n",
      "Skipping line 226: expected 3 fields, saw 5\n",
      "Skipping line 227: expected 3 fields, saw 5\n",
      "Skipping line 228: expected 3 fields, saw 5\n",
      "Skipping line 229: expected 3 fields, saw 5\n",
      "Skipping line 230: expected 3 fields, saw 5\n",
      "Skipping line 231: expected 3 fields, saw 5\n",
      "Skipping line 232: expected 3 fields, saw 5\n",
      "Skipping line 233: expected 3 fields, saw 5\n",
      "Skipping line 234: expected 3 fields, saw 5\n",
      "Skipping line 235: expected 3 fields, saw 5\n",
      "Skipping line 236: expected 3 fields, saw 5\n",
      "Skipping line 237: expected 3 fields, saw 5\n",
      "Skipping line 238: expected 3 fields, saw 5\n",
      "Skipping line 239: expected 3 fields, saw 5\n",
      "Skipping line 240: expected 3 fields, saw 5\n",
      "Skipping line 241: expected 3 fields, saw 5\n",
      "Skipping line 242: expected 3 fields, saw 5\n",
      "Skipping line 243: expected 3 fields, saw 5\n",
      "Skipping line 244: expected 3 fields, saw 5\n",
      "Skipping line 245: expected 3 fields, saw 5\n",
      "Skipping line 246: expected 3 fields, saw 5\n",
      "Skipping line 247: expected 3 fields, saw 5\n",
      "Skipping line 248: expected 3 fields, saw 5\n",
      "Skipping line 249: expected 3 fields, saw 5\n",
      "Skipping line 250: expected 3 fields, saw 5\n",
      "Skipping line 251: expected 3 fields, saw 5\n",
      "Skipping line 252: expected 3 fields, saw 5\n",
      "Skipping line 253: expected 3 fields, saw 5\n",
      "Skipping line 254: expected 3 fields, saw 5\n",
      "Skipping line 255: expected 3 fields, saw 5\n",
      "Skipping line 256: expected 3 fields, saw 5\n",
      "Skipping line 257: expected 3 fields, saw 5\n",
      "Skipping line 258: expected 3 fields, saw 5\n",
      "Skipping line 259: expected 3 fields, saw 5\n",
      "Skipping line 260: expected 3 fields, saw 5\n",
      "Skipping line 261: expected 3 fields, saw 5\n",
      "Skipping line 262: expected 3 fields, saw 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, output_csv_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        all_tables = []\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                # Ensuring that each row in the table has the same number of columns\n",
    "                if all(len(row) == len(tables[0][0]) for row in table):\n",
    "                    all_tables.extend(table)\n",
    "                else:\n",
    "                    print(\"Skipped a malformed table\")\n",
    "        with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for row in all_tables:\n",
    "                writer.writerow(row)\n",
    "\n",
    "def process_csv_data(output_csv_path):\n",
    "    try:\n",
    "        # Attempt to read the CSV with a fixed number of columns to avoid skipping lines\n",
    "        data_df_one = pd.read_csv(output_csv_path, header=None, names=['Column1', 'Column2', 'Column3'], on_bad_lines='warn')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read CSV: {str(e)}\")\n",
    "\n",
    "    return data_df_one\n",
    "\n",
    "# Specify paths\n",
    "pdf_path = 'Data/ToF-3Aug21FinalComb.pdf'\n",
    "output_csv_path = 'Data/2021_Fees.csv'\n",
    "\n",
    "# Extract tables from PDF\n",
    "extract_tables_from_pdf(pdf_path, output_csv_path)\n",
    "\n",
    "# Process CSV data\n",
    "data_df_one = process_csv_data(output_csv_path)\n",
    "\n",
    "# Debugging: Output the first few lines to check if they are read correctly\n",
    "print(data_df_one.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
